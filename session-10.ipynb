{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 10: Prompt Logic and Human-AI Interaction\n",
    "## Controlling Models Through Design\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Master prompt engineering -- the skill of designing inputs to get better outputs from AI models. Then use everything you have learned to plan your independent project.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What pipeline designs did you sketch? |\n",
    "| 0:05-0:30 | Part 1: Prompts as Model Control |\n",
    "| 0:30-1:00 | Part 2: Designing a Tool (live build with AI) |\n",
    "| 1:00-1:40 | Part 3: Project Planning |\n",
    "| 1:40-2:00 | On Your Own: Start building your project prototype |\n",
    "\n",
    "### Key Vocabulary\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| Prompt Engineering | Designing inputs to get better outputs from models |\n",
    "| CLEAR Framework | Context, Language, Explain, Ask, Requirements |\n",
    "| Zero-Shot Prompting | Giving a model a task without examples |\n",
    "| Candidate Labels | The categories you give a zero-shot model to choose from |\n",
    "| Tool Design | Planning how an AI tool should work for its users |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What Pipeline Designs Did You Sketch? (0:00-0:05)\n",
    "\n",
    "Last session we built multi-model pipelines -- chaining models so the output of one feeds into the next. We saw how error cascades can ruin downstream results, and we designed pipelines on paper.\n",
    "\n",
    "Today we focus on the **control layer** -- how the inputs you design determine the outputs you get. In Session 5, you learned about temperature and top_p. Those are numerical controls. But the most powerful control is the **prompt itself**.\n",
    "\n",
    "Now that you understand training data (Session 4), bias (Session 8), and generalization (Session 7), you can write better prompts. Let's see how.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install transformers==4.47.1 -q"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart Your Runtime\n",
    "\n",
    "After installing packages, you need to restart the runtime so Python can find them.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "After restarting, come back here and continue running the cells below. You do NOT need to re-run the install cell -- the packages are already installed. Just start from the next code cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Prompts as Model Control (0:05-0:30)\n",
    "\n",
    "The zero-shot classifier does not look at code. It looks at **the labels you give it**. Change the labels, change the results. This makes the labels a form of **prompt engineering** -- you are controlling the model's behavior through your input design.\n",
    "\n",
    "### Experiment 1: Vague vs. Specific Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "print(\"Classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I spent three hours on this assignment and I'm exhausted but proud of the result\"\n",
    "\n",
    "# Vague categories\n",
    "vague_labels = [\"good\", \"bad\"]\n",
    "result_vague = classifier(text, vague_labels)\n",
    "\n",
    "# Specific categories\n",
    "specific_labels = [\"productive effort\", \"frustration\", \"accomplishment\", \"burnout\"]\n",
    "result_specific = classifier(text, specific_labels)\n",
    "\n",
    "print(\"VAGUE vs. SPECIFIC CATEGORIES\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "print(\"Vague labels:\")\n",
    "for label, score in zip(result_vague['labels'], result_vague['scores']):\n",
    "    print(f\"  {label}: {score:.1%}\")\n",
    "\n",
    "print(\"\\nSpecific labels:\")\n",
    "for label, score in zip(result_specific['labels'], result_specific['scores']):\n",
    "    print(f\"  {label}: {score:.1%}\")\n",
    "\n",
    "print(\"\\nSame text, completely different analysis -- just by changing the labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vague labels force a binary choice that misses the complexity. The specific labels capture the nuance -- this text contains BOTH exhaustion AND pride. **Better labels produce better insights.**\n",
    "\n",
    "### Experiment 2: One-Word vs. Descriptive Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The new policy will increase costs for small businesses but could create more jobs overall\"\n",
    "\n",
    "# One-word labels\n",
    "one_word = [\"positive\", \"negative\", \"neutral\"]\n",
    "result_short = classifier(text, one_word)\n",
    "\n",
    "# Descriptive labels\n",
    "descriptive = [\n",
    "    \"mostly beneficial with some drawbacks\",\n",
    "    \"mostly harmful with some benefits\",\n",
    "    \"mixed impact with trade-offs\",\n",
    "    \"insufficient information to judge\"\n",
    "]\n",
    "result_desc = classifier(text, descriptive)\n",
    "\n",
    "print(\"ONE-WORD vs. DESCRIPTIVE LABELS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "print(\"One-word labels:\")\n",
    "for label, score in zip(result_short['labels'], result_short['scores']):\n",
    "    print(f\"  {label}: {score:.1%}\")\n",
    "\n",
    "print(\"\\nDescriptive labels:\")\n",
    "for label, score in zip(result_desc['labels'], result_desc['scores']):\n",
    "    print(f\"  {label}: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive labels give the model more context about what each category actually means. This is prompt engineering at its core: **the more precisely you describe what you want, the more precisely the model can deliver.**\n",
    "\n",
    "### Experiment 3: Student-Designed Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to suggest a text and then design their own set of labels. Run it and discuss whether the labels captured what they wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_text = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "student_categories = [\"REPLACE\", \"WITH\", \"YOUR\", \"OWN\"]\n",
    "\n",
    "if \"REPLACE\" not in student_text and \"REPLACE\" not in student_categories[0]:\n",
    "    result = classifier(student_text, student_categories)\n",
    "    print(f\"Text: {student_text}\\n\")\n",
    "    print(\"Your categories:\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  {label}: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Prompt Phrasing for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(\"Generator loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same topic, different prompt framings\n",
    "prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The most important thing about artificial intelligence is\",\n",
    "    \"A scientist explaining artificial intelligence to a child would say\",\n",
    "    \"The danger of artificial intelligence is\",\n",
    "    \"The promise of artificial intelligence is\",\n",
    "]\n",
    "\n",
    "print(\"PROMPT FRAMING EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generator(prompt, max_length=50, do_sample=True, temperature=0.8)\n",
    "    output = result[0]['generated_text'][len(prompt):].strip()[:60]\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"  --> {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same topic, five completely different directions -- all controlled by how you frame the prompt. The prompt is not just input. It is **steering**.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "| What You Learned | Session | How It Connects to Prompts |\n",
    "|-----------------|---------|---------------------------|\n",
    "| Models have training domains | Session 7 | Use language that matches the training domain |\n",
    "| Models have biases | Session 8 | Test prompts for biased framing |\n",
    "| Labels change classification | Session 5 | Design labels carefully |\n",
    "| Error cascades | Session 9 | Bad prompts cascade into bad outputs |\n",
    "\n",
    "Prompt engineering is not a trick. It is **applied understanding of how models work**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the vague vs. specific experiment into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"Why do more specific category labels produce better results in zero-shot classification? What is the model actually doing with these labels internally?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Designing a Tool (0:30-1:00)\n",
    "\n",
    "You know what models can do. You know their limits. You know how to chain them. You know how to control them with prompts. Now: **what do you want to BUILD?**\n",
    "\n",
    "### The CLEAR Framework (Revisited)\n",
    "\n",
    "In Session 3, you learned the CLEAR framework for asking AI to write code:\n",
    "\n",
    "| Letter | Meaning | Example |\n",
    "|--------|---------|---------|\n",
    "| **C** | Context | \"I'm working in Google Colab with Python\" |\n",
    "| **L** | Language/Libraries | \"Using the Hugging Face transformers library\" |\n",
    "| **E** | Explain the goal | \"I want to build a tool that analyzes customer reviews\" |\n",
    "| **A** | Ask specifically | \"Write a function that takes a list of reviews and returns sentiment + emotion for each\" |\n",
    "| **R** | Requirements | \"Include comments, handle empty input, print a summary at the end\" |\n",
    "\n",
    "Back then, you were learning the basics. Now you have 9 sessions of knowledge about what models can do, how they fail, and how to control them. Your CLEAR prompts can be **much more sophisticated**.\n",
    "\n",
    "### Live Demo: Building a Tool With AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** This is the core exercise. Open Claude or ChatGPT on screen. Ask a student to describe a tool they want to build. Write a CLEAR prompt WITH the class -- have them suggest each piece. Paste the code into Colab. Run it. Debug if needed. The entire Part 2 IS this demo.\n",
    ">\n",
    "> **Example CLEAR prompt the class might write together:**\n",
    ">\n",
    "> \"I'm working in Google Colab with Python. Using the Hugging Face transformers library, write a function called `analyze_feedback` that:\n",
    "> - Takes a list of student feedback comments as input\n",
    "> - Uses sentiment analysis to classify each as positive/negative\n",
    "> - Uses zero-shot classification with labels [\"praise\", \"suggestion\", \"complaint\", \"question\"]\n",
    "> - Prints each comment with its sentiment and category\n",
    "> - Prints a summary at the end (counts per category, overall sentiment distribution)\n",
    "> - Include comments explaining each step\"\n",
    ">\n",
    "> Type the AI-generated code in the cell below and run it live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASTE AI-GENERATED CODE HERE\n",
    "# (Instructor: paste the code that Claude/ChatGPT generates from the class CLEAR prompt)\n",
    "\n",
    "# Placeholder example -- replace with the live-generated code:\n",
    "def analyze_feedback(comments):\n",
    "    \"\"\"Analyze student feedback using sentiment + zero-shot classification.\"\"\"\n",
    "    sent = pipeline(\"sentiment-analysis\")\n",
    "    clf = pipeline(\"zero-shot-classification\")\n",
    "    categories = [\"praise\", \"suggestion\", \"complaint\", \"question\"]\n",
    "\n",
    "    print(\"FEEDBACK ANALYSIS\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    cat_counts = {c: 0 for c in categories}\n",
    "    sent_counts = {\"POSITIVE\": 0, \"NEGATIVE\": 0}\n",
    "\n",
    "    for comment in comments:\n",
    "        s = sent(comment)[0]\n",
    "        c = clf(comment, categories)\n",
    "        top_cat = c['labels'][0]\n",
    "        cat_counts[top_cat] += 1\n",
    "        sent_counts[s['label']] += 1\n",
    "\n",
    "        print(f\"\\n\\\"{comment}\\\"\")\n",
    "        print(f\"  Sentiment: {s['label']} ({s['score']:.0%})\")\n",
    "        print(f\"  Category: {top_cat} ({c['scores'][0]:.0%})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 55)\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"  Total comments: {len(comments)}\")\n",
    "    for cat, count in cat_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {cat}: {count}\")\n",
    "    print(f\"  Positive: {sent_counts['POSITIVE']}, Negative: {sent_counts['NEGATIVE']}\")\n",
    "\n",
    "# Test it\n",
    "sample_feedback = [\n",
    "    \"The examples in class were really helpful!\",\n",
    "    \"Could we get more time for the group project?\",\n",
    "    \"I didn't understand the homework instructions at all.\",\n",
    "    \"Is the final exam cumulative or just recent material?\",\n",
    "    \"Best class I've taken this year, seriously.\",\n",
    "]\n",
    "\n",
    "analyze_feedback(sample_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** After running the tool, discuss with the class:\n",
    "> - What worked well?\n",
    "> - What would you change?\n",
    "> - How could we improve the CLEAR prompt to get better code?\n",
    "> - What would happen if we changed the categories?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Project Planning (1:00-1:25)\n",
    "\n",
    "By the end of this section, each student should have a **1-sentence project description** and a plan for what to build.\n",
    "\n",
    "### What You Know Now\n",
    "\n",
    "Over 10 sessions, you have learned:\n",
    "\n",
    "| Session | Skill |\n",
    "|---------|-------|\n",
    "| 1 | INPUT --> MODEL --> OUTPUT pattern |\n",
    "| 2 | Running models with code (pipelines) |\n",
    "| 3 | Using AI to help you code (CLEAR framework) |\n",
    "| 4 | How models learn (supervised learning, training data) |\n",
    "| 5 | Controlling models (temperature, top-p, labels) |\n",
    "| 6 | Evaluating models (confidence, accuracy, false positives) |\n",
    "| 7 | Domain shift and generalization |\n",
    "| 8 | Bias, uncertainty, and real-world consequences |\n",
    "| 9 | Multi-model pipelines and error cascades |\n",
    "| 10 | Prompt engineering and tool design |\n",
    "\n",
    "You are ready to build something real.\n",
    "\n",
    "### Project Spec Template\n",
    "\n",
    "Fill this out. Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Project Name:**\n",
    "\n",
    "**One-sentence description:** (What does it do, and who is it for?)\n",
    "\n",
    "**What models will it use?**\n",
    "- Model 1: _________ (for _________)\n",
    "- Model 2: _________ (for _________)\n",
    "- Model 3: _________ (for _________) [optional]\n",
    "\n",
    "**What is the input?**\n",
    "\n",
    "**What is the output?**\n",
    "\n",
    "**One thing it probably WON'T handle well:** (Connect to Sessions 7-8 -- what domain shift or bias issues might affect your tool?)\n",
    "\n",
    "**My CLEAR prompt to start building:**\n",
    "```\n",
    "C - Context:\n",
    "L - Language/Libraries:\n",
    "E - Explain the goal:\n",
    "A - Ask specifically:\n",
    "R - Requirements:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Have each student share their 1-sentence project description with the group. This is a commitment point -- by next session, they should have something that runs.\n",
    "\n",
    "### Browse Models for Your Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Have students go to huggingface.co/models and search for models relevant to their project. Show them how to filter by task (text-classification, summarization, etc.) and sort by downloads. Point out that popular models are popular for a reason -- they tend to be more reliable.\n",
    "\n",
    "### Available Models Reference\n",
    "\n",
    "Here are the models you have used in this course (all work on free Colab CPU):\n",
    "\n",
    "**Text Models:**\n",
    "| Pipeline | Model | What It Does |\n",
    "|----------|-------|-------------|\n",
    "| `sentiment-analysis` | (default) | Positive/negative sentiment |\n",
    "| `sentiment-analysis` | `cardiffnlp/twitter-roberta-base-sentiment-latest` | Twitter-optimized sentiment |\n",
    "| `sentiment-analysis` | `nlptown/bert-base-multilingual-uncased-sentiment` | 1-5 star ratings |\n",
    "| `text-classification` | `j-hartmann/emotion-english-distilroberta-base` | Emotion detection (joy, anger, etc.) |\n",
    "| `zero-shot-classification` | (default) | Classify into ANY categories you define |\n",
    "| `question-answering` | (default) | Extract answers from a passage |\n",
    "| `summarization` | `sshleifer/distilbart-cnn-12-6` | Summarize long text |\n",
    "| `ner` | (default, `grouped_entities=True`) | Find people, places, organizations |\n",
    "| `text-generation` | `distilgpt2` | Generate text from a prompt |\n",
    "\n",
    "**Image Models:**\n",
    "| Pipeline | Model | What It Does |\n",
    "|----------|-------|-------------|\n",
    "| `image-to-text` | `Salesforce/blip-image-captioning-base` | Describe an image in words |\n",
    "| `zero-shot-image-classification` | `openai/clip-vit-base-patch32` | Classify images into ANY categories |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Describe your project idea to Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"I want to build [your idea] using Hugging Face models in Google Colab. What models would you recommend? What order should they run in? What problems should I watch out for?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Start Building Your Project Prototype\n",
    "\n",
    "Use the CLEAR framework to ask Claude or ChatGPT for starter code. Paste it here and try to get something running before next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PROJECT: STARTER CODE\n",
    "# Paste AI-generated code here and test it\n",
    "\n",
    "# Step 1: Load your models\n",
    "# from transformers import pipeline\n",
    "# model_1 = pipeline(\"...\")\n",
    "# model_2 = pipeline(\"...\")\n",
    "\n",
    "# Step 2: Your main function\n",
    "def my_project(input_data):\n",
    "    \"\"\"\n",
    "    REPLACE: Describe what your project does.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 3: Test it\n",
    "# my_project(\"test input here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT TESTING CELL\n",
    "# Try different inputs to see how your tool behaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between Sessions\n",
    "\n",
    "Come to Session 11 with **something that runs** -- even if it is rough. It does not need to be perfect. It needs to work.\n",
    "\n",
    "Use Claude or ChatGPT with the CLEAR framework to help you build. That is not cheating -- it is exactly the skill we have been practicing.\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist: Before You Leave\n",
    "\n",
    "- [ ] Saw how label design changes zero-shot classification results\n",
    "- [ ] Understood that prompt framing steers text generation\n",
    "- [ ] Watched (or participated in) a live tool-building exercise with AI\n",
    "- [ ] Wrote a 1-sentence project description\n",
    "- [ ] Filled out the project spec template\n",
    "- [ ] Browsed Hugging Face models for your project\n",
    "- [ ] Started (or have a plan for) your project prototype\n",
    "- [ ] Saved your work (File > Save a copy in Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking Ahead\n",
    "\n",
    "Next session is **project work time**. You will share what you built, get feedback, iterate, and document. Come with something that runs -- even if it is messy. Session 11 is about making it better, not starting from scratch.\n",
    "\n",
    "See you next session.\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}