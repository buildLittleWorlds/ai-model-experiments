{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6: Model Evaluation and Metrics\n",
    "## How Do You Know If a Model Is Good?\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Learn to evaluate AI models critically -- understand confidence scores, compare models head-to-head, and discover why accuracy alone can be misleading.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What did parameter experiments reveal? |\n",
    "| 0:05-0:30 | Part 1: Confidence Scores -- Can You Trust Them? |\n",
    "| 0:30-1:05 | Part 2: Model Comparison -- Same Input, Different Models |\n",
    "| 1:05-1:40 | Part 3: When Numbers Lie |\n",
    "| 1:40-2:00 | On Your Own: Extended model comparison |\n",
    "\n",
    "### Key Vocabulary\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| Confidence Score | A number (0-1) showing how sure the model is |\n",
    "| Model Comparison | Testing the same input on different models to see who is better |\n",
    "| Accuracy | How often the model gets the right answer |\n",
    "| False Positive | When the model says YES but the answer is NO |\n",
    "| Evaluation | Systematically testing how well a model performs |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What Did Parameter Experiments Reveal? (0:00-0:05)\n",
    "\n",
    "Last session we learned that models have **hyperparameters** -- dials you control that change how the model behaves. Temperature controls creativity, top-p limits word choices, max_length controls summary size, and even the labels you give a zero-shot classifier change its results.\n",
    "\n",
    "Today we ask the next logical question: **how do you know if a model is actually good?**\n",
    "\n",
    "A model can generate text, classify sentiment, summarize articles. But is it doing a good job? How would you even measure that? And what happens when the numbers say \"great\" but the model is actually failing?\n",
    "\n",
    "Let's find out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.47.1 gradio -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart Your Runtime\n",
    "\n",
    "After installing packages, you need to restart the runtime so Python can find them.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "After restarting, come back here and continue running the cells below. You do NOT need to re-run the install cell -- the packages are already installed. Just start from the next code cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Confidence Scores -- Can You Trust Them? (0:05-0:30)\n",
    "\n",
    "Every time a classification model makes a prediction, it also gives you a **confidence score** -- a number between 0 and 1 that represents how sure the model is. A score of 0.95 means \"I'm 95% confident in this answer.\"\n",
    "\n",
    "But here is the critical question: **does confident mean correct?**\n",
    "\n",
    "### Sentiment Analysis: The Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(\"Sentiment model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy Cases: High Confidence, Correct Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_texts = [\n",
    "    \"I absolutely love this movie, it was incredible!\",\n",
    "    \"This is the worst product I have ever purchased.\",\n",
    "    \"The concert was fantastic and everyone had a great time.\"\n",
    "]\n",
    "\n",
    "print(\"EASY CASES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in easy_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprises here. Clear positive text gets classified as positive with high confidence. Clear negative text gets classified as negative. The model is sure, and it is right.\n",
    "\n",
    "But what happens when the text is not so clear?\n",
    "\n",
    "#### Ambiguous Cases: Lower Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_texts = [\n",
    "    \"It was fine.\",\n",
    "    \"I guess it could have been worse.\",\n",
    "    \"The food was okay but the service was slow.\",\n",
    "    \"Not bad, not great, just average.\"\n",
    "]\n",
    "\n",
    "print(\"AMBIGUOUS CASES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in ambiguous_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the confidence drops for ambiguous text. The model is less sure -- and that is actually appropriate. When the text genuinely could go either way, a lower confidence score is honest.\n",
    "\n",
    "#### The Dangerous Case: High Confidence, Wrong Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Before running the next cell, ask students: \"What do you think will happen with sarcasm? The model has never been taught about sarcasm -- it just looks at word patterns.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_texts = [\n",
    "    \"Oh wonderful, another meeting. Exactly how I wanted to spend my Saturday.\",\n",
    "    \"What a great idea to release the product without testing it first.\",\n",
    "    \"I just love sitting in traffic for two hours. Best part of my day.\",\n",
    "    \"Sure, because that worked so well last time.\"\n",
    "]\n",
    "\n",
    "print(\"TRICKY CASES: SARCASM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in tricky_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dangerous scenario. The model sees words like \"wonderful,\" \"great,\" \"love,\" and \"best\" and confidently predicts POSITIVE. But a human reader immediately recognizes the sarcasm -- the actual sentiment is negative.\n",
    "\n",
    "**A confidence score tells you how sure the model is. It does NOT tell you whether the model is correct.** A model can be 99% confident and 100% wrong. This is one of the most important lessons in AI evaluation.\n",
    "\n",
    "### Zero-Shot on Genuinely Ambiguous Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "text = \"I spent three hours working on this project\"\n",
    "categories = [\"productive\", \"frustrated\", \"neutral\", \"dedicated\"]\n",
    "\n",
    "result = classifier(text, categories)\n",
    "\n",
    "print(\"AMBIGUOUS TEXT: ZERO-SHOT CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students: \"Is 'I spent three hours working on this project' positive or negative? Could it be both? What confidence score would you trust -- 90%? 70%? 51%?\"\n",
    "\n",
    "### Student Test: Find a Confidence Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to suggest ambiguous or sarcastic text. Type it below and see if the model gets fooled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_ambiguous = \"REPLACE WITH SOMETHING AMBIGUOUS\"\n",
    "\n",
    "result = sentiment(student_ambiguous)[0]\n",
    "print(f\"Text: {student_ambiguous}\")\n",
    "print(f\"Prediction: {result['label']} (confidence: {result['score']:.1%})\")\n",
    "\n",
    "# Also try zero-shot\n",
    "zs_result = classifier(student_ambiguous, [\"positive\", \"negative\", \"sarcastic\", \"neutral\"])\n",
    "print(f\"\\nZero-shot breakdown:\")\n",
    "for label, score in zip(zs_result['labels'], zs_result['scores']):\n",
    "    print(f\"  {label}: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the sarcasm examples and their predictions into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"These three models gave different answers for the same text. Why might that happen? What is different about these models?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Comparison -- Same Input, Different Models (0:30-1:05)\n",
    "\n",
    "Here is a fact that surprises most people: **different models give different answers for the same input.** Not slightly different -- sometimes completely opposite answers.\n",
    "\n",
    "Why? Because each model was trained on different data. A model trained on movie reviews has a different idea of \"positive\" than a model trained on tweets. A model trained on product reviews might not understand political text at all.\n",
    "\n",
    "Let's see this in action by running the same sentences through three different sentiment models.\n",
    "\n",
    "### Loading Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Default distilbert (trained on movie reviews - SST-2)\n",
    "model_a = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Model B: Twitter-specific model (trained on tweets)\n",
    "model_b = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Model C: Star-rating model (trained on product reviews, 1-5 stars)\n",
    "model_c = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "print(\"All three models loaded!\")\n",
    "print()\n",
    "print(\"Model A: distilbert-base (movie reviews)\")\n",
    "print(\"Model B: twitter-roberta (tweets)\")\n",
    "print(\"Model C: bert-multilingual (product reviews, 1-5 stars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Comparison Test\n",
    "\n",
    "We will run the same sentences through all three models and see where they agree and disagree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Let students suggest 2-3 test sentences. Replace the \"REPLACE WITH STUDENT SUGGESTION\" entries with their ideas before running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"I absolutely love this product!\",\n",
    "    \"This is the worst experience I have ever had.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"Oh great, another update that breaks everything.\",\n",
    "    \"The service was slow but the food was amazing.\",\n",
    "    \"I didn't hate it.\",\n",
    "    \"REPLACE WITH STUDENT SUGGESTION\",\n",
    "    \"REPLACE WITH STUDENT SUGGESTION\",\n",
    "]\n",
    "\n",
    "print(f\"{'Text':<45} {'Default':<15} {'Twitter':<15} {'Stars':<15}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for text in test_sentences:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    a = model_a(text)[0]\n",
    "    b = model_b(text)[0]\n",
    "    c = model_c(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    c_str = f\"{c['label'][:8]} {c['score']:.0%}\"\n",
    "    print(f\"{text[:43]:<45} {a_str:<15} {b_str:<15} {c_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions\n",
    "\n",
    "Look at the comparison table and think about these:\n",
    "\n",
    "1. **Where did all three models agree?** These are probably \"easy\" cases where the sentiment is obvious.\n",
    "\n",
    "2. **Where did they disagree?** These are the interesting cases. Why might the Twitter model see something differently than the movie review model?\n",
    "\n",
    "3. **The Twitter model was trained on tweets. The default model was trained on movie reviews.** How might that explain their different responses to sarcasm or casual language?\n",
    "\n",
    "4. **The star-rating model gives 1-5 stars instead of positive/negative.** When is a 3-star rating useful information that binary positive/negative misses?\n",
    "\n",
    "### Detailed Breakdown\n",
    "\n",
    "Let's look at one sentence in detail across all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_text = \"The service was slow but the food was amazing.\"\n",
    "\n",
    "print(f\"Text: {detail_text}\")\n",
    "print()\n",
    "\n",
    "print(\"Model A (Default - Movie Reviews):\")\n",
    "a = model_a(detail_text)[0]\n",
    "print(f\"  {a['label']} ({a['score']:.1%})\")\n",
    "\n",
    "print(\"\\nModel B (Twitter):\")\n",
    "b = model_b(detail_text)[0]\n",
    "print(f\"  {b['label']} ({b['score']:.1%})\")\n",
    "\n",
    "print(\"\\nModel C (Product Reviews - Stars):\")\n",
    "c = model_c(detail_text)[0]\n",
    "print(f\"  {c['label']} ({c['score']:.1%})\")\n",
    "\n",
    "print()\n",
    "print(\"This sentence has MIXED sentiment -- one bad thing and one good thing.\")\n",
    "print(\"Notice how each model handles the contradiction differently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Go to huggingface.co/models and filter by \"text-classification\". Show students how many sentiment models exist (hundreds). Click on cardiffnlp/twitter-roberta-base-sentiment-latest -- show the model card, training data section, and any benchmark numbers. Point out that the model card tells you what data the model was trained on, which helps explain its behavior.\n",
    "\n",
    "### Student Challenge: Find the Biggest Disagreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to come up with sentences where they think the models will disagree the most. Test their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_challenge = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "print(f\"Text: {student_challenge}\\n\")\n",
    "\n",
    "a = model_a(student_challenge)[0]\n",
    "b = model_b(student_challenge)[0]\n",
    "c = model_c(student_challenge)[0]\n",
    "\n",
    "print(f\"Default:  {a['label']} ({a['score']:.1%})\")\n",
    "print(f\"Twitter:  {b['label']} ({b['score']:.1%})\")\n",
    "print(f\"Stars:    {c['label']} ({c['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the comparison table output into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"These three sentiment models gave different answers for the same texts. One was trained on movie reviews, one on tweets, and one on product reviews. Explain why training data affects predictions, using these results as examples.\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: When Numbers Lie (1:05-1:25)\n",
    "\n",
    "We have seen that confidence scores can be misleading and that different models disagree. Now let's talk about the most common trap in AI evaluation: **accuracy that hides failure.**\n",
    "\n",
    "### The Spam Detector Thought Experiment\n",
    "\n",
    "Imagine you build a spam detector for email. You test it on 1,000 emails:\n",
    "- 950 are real emails (not spam)\n",
    "- 50 are spam\n",
    "\n",
    "Your model predicts \"NOT SPAM\" for every single email. Every one. It never flags anything as spam.\n",
    "\n",
    "**Quiz: What is this model's accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_emails = 1000\n",
    "real_emails = 950\n",
    "spam_emails = 50\n",
    "\n",
    "# The model predicts NOT SPAM for everything\n",
    "correct_predictions = real_emails  # It gets all 950 real emails right\n",
    "wrong_predictions = spam_emails    # It misses all 50 spam emails\n",
    "\n",
    "accuracy = correct_predictions / total_emails\n",
    "\n",
    "print(\"THE LAZY SPAM DETECTOR\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total emails tested: {total_emails}\")\n",
    "print(f\"Real emails:         {real_emails}\")\n",
    "print(f\"Spam emails:         {spam_emails}\")\n",
    "print()\n",
    "print(f\"Model predicts: NOT SPAM for everything\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Wrong predictions:   {wrong_predictions}\")\n",
    "print(f\"Accuracy:            {accuracy:.1%}\")\n",
    "print()\n",
    "print(\"95% accuracy! Ship it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**95% accuracy.** That sounds great -- until you realize the model is completely useless. It never catches a single spam email. Its entire job is to find spam, and it finds zero.\n",
    "\n",
    "This is why **accuracy alone hides problems.** A model can be \"accurate\" while failing completely at its actual job. The 95% accuracy comes from a trick: since most emails are real, the model can just predict \"not spam\" every time and be right 95% of the time.\n",
    "\n",
    "### What Went Wrong: The Imbalanced Data Problem\n",
    "\n",
    "When one category is much more common than another (950 real vs. 50 spam), the model can cheat by always guessing the common category. This is called the **class imbalance problem**, and it is one of the most common traps in machine learning.\n",
    "\n",
    "### A Better Way to Evaluate\n",
    "\n",
    "Instead of just asking \"how often is the model right?\", we should ask:\n",
    "- **Of the emails the model flagged as spam, how many actually were spam?** (Precision)\n",
    "- **Of all the actual spam emails, how many did the model catch?** (Recall)\n",
    "\n",
    "Let's see this with a simple simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TWO SPAM DETECTORS COMPARED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Detector A: predicts NOT SPAM for everything\n",
    "print(\"\\nDetector A: Predicts NOT SPAM for everything\")\n",
    "print(f\"  Accuracy: 95.0%\")\n",
    "print(f\"  Spam caught: 0 out of 50 (0%)\")\n",
    "print(f\"  Verdict: USELESS -- never catches spam\")\n",
    "\n",
    "# Detector B: catches some spam, makes some mistakes\n",
    "print(\"\\nDetector B: Actually tries to detect spam\")\n",
    "print(f\"  Accuracy: 92.0%\")\n",
    "print(f\"  Spam caught: 40 out of 50 (80%)\")\n",
    "print(f\"  False alarms: 30 real emails flagged as spam\")\n",
    "print(f\"  Verdict: USEFUL -- catches most spam, some false alarms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nDetector A has HIGHER accuracy (95% vs 92%)\")\n",
    "print(\"But Detector B is clearly the better spam detector.\")\n",
    "print(\"\\nThis is why you cannot evaluate a model on accuracy alone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positives and False Negatives\n",
    "\n",
    "When a model makes a mistake, there are two kinds of errors:\n",
    "\n",
    "| Error Type | What Happened | Spam Example |\n",
    "|-----------|---------------|--------------|\n",
    "| **False Positive** | Model said YES, answer was NO | Flagged a real email as spam |\n",
    "| **False Negative** | Model said NO, answer was YES | Let a spam email through to inbox |\n",
    "\n",
    "Which error is worse depends on the task:\n",
    "- **Medical diagnosis:** A false negative (missing a disease) is much worse than a false positive (unnecessary test).\n",
    "- **Spam filter:** A false positive (blocking a real email) might be worse than a false negative (letting some spam through).\n",
    "- **Self-driving car:** A false negative (not seeing a pedestrian) is catastrophic.\n",
    "\n",
    "There is no universal answer to \"which error is worse.\" It depends entirely on what the model is being used for. This is a **design decision**, not a math problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing It With Real Models\n",
    "\n",
    "Let's create a small evaluation experiment with our sentiment model. We will test it on sentences where we know the right answer and see where it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases where WE know the correct sentiment\n",
    "test_cases = [\n",
    "    (\"I love this!\", \"POSITIVE\"),\n",
    "    (\"Terrible product.\", \"NEGATIVE\"),\n",
    "    (\"Best day ever!\", \"POSITIVE\"),\n",
    "    (\"Total waste of money.\", \"NEGATIVE\"),\n",
    "    (\"It was okay.\", \"POSITIVE\"),    # Arguable -- could go either way\n",
    "    (\"Not my favorite.\", \"NEGATIVE\"),\n",
    "    (\"Could be better.\", \"NEGATIVE\"),\n",
    "    (\"I didn't hate it.\", \"POSITIVE\"),  # Tricky -- weak positive\n",
    "    (\"What a disaster.\", \"NEGATIVE\"),\n",
    "    (\"Pretty good actually.\", \"POSITIVE\"),\n",
    "]\n",
    "\n",
    "print(\"SENTIMENT MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Text':<30} {'Expected':<12} {'Predicted':<12} {'Match?':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "correct = 0\n",
    "total = len(test_cases)\n",
    "\n",
    "for text, expected in test_cases:\n",
    "    result = sentiment(text)[0]\n",
    "    predicted = result['label']\n",
    "    match = predicted == expected\n",
    "    correct += int(match)\n",
    "    marker = \"YES\" if match else \"NO <--\"\n",
    "    print(f\"{text:<30} {expected:<12} {predicted:<12} {marker}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Accuracy: {correct}/{total} = {correct/total:.0%}\")\n",
    "print()\n",
    "if correct < total:\n",
    "    print(f\"The model got {total - correct} wrong. Look at the misses --\")\n",
    "    print(\"are they cases where the 'right' answer is genuinely debatable?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Ask Claude or ChatGPT:\n",
    ">\n",
    "> *\"A spam detection model that always predicts NOT SPAM has 95% accuracy because 95% of emails are real. Explain why this is misleading. What metrics would be better, and why?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "### Preview: What Comes Next\n",
    "\n",
    "This is why real data scientists never report just accuracy. They look at:\n",
    "- Where does the model fail?\n",
    "- What kinds of mistakes does it make?\n",
    "- Are the mistakes acceptable for this use case?\n",
    "\n",
    "Next session, we will go deeper into what happens when the data the model sees in the real world looks different from what it trained on. A model trained on movie reviews might fail on tweets. A model trained on English might fail on slang. Understanding these failure modes is what separates someone who uses AI from someone who understands AI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Experiment 1: Build Your Own Test Suite\n",
    "\n",
    "Add your own test sentences to the comparison. Try to find inputs where all three models give different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_sentences = [\n",
    "    \"REPLACE WITH YOUR SENTENCE 1\",\n",
    "    \"REPLACE WITH YOUR SENTENCE 2\",\n",
    "    \"REPLACE WITH YOUR SENTENCE 3\",\n",
    "    \"REPLACE WITH YOUR SENTENCE 4\",\n",
    "    \"REPLACE WITH YOUR SENTENCE 5\",\n",
    "]\n",
    "\n",
    "print(f\"{'Text':<45} {'Default':<15} {'Twitter':<15} {'Stars':<15}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for text in my_test_sentences:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    a = model_a(text)[0]\n",
    "    b = model_b(text)[0]\n",
    "    c = model_c(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    c_str = f\"{c['label'][:8]} {c['score']:.0%}\"\n",
    "    print(f\"{text[:43]:<45} {a_str:<15} {b_str:<15} {c_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Which Model Would You Pick?\n",
    "\n",
    "Imagine you are building a sentiment analysis tool for one of these use cases. Which of the three models would you choose, and why?\n",
    "\n",
    "| Use Case | Model Choice | Why? |\n",
    "|----------|-------------|------|\n",
    "| Analyzing customer reviews for a restaurant | | |\n",
    "| Monitoring Twitter for brand mentions | | |\n",
    "| Scanning movie reviews for a recommendation app | | |\n",
    "| Checking student feedback on a class | | |\n",
    "\n",
    "There is no single right answer. The best model depends on the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Create Your Own Evaluation\n",
    "\n",
    "Pick a topic you care about and write 10 test sentences where YOU know the correct sentiment. Run them through the model and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own test cases: (text, expected_label)\n",
    "# Expected label should be \"POSITIVE\" or \"NEGATIVE\"\n",
    "my_evaluation = [\n",
    "    (\"REPLACE WITH YOUR TEXT\", \"POSITIVE\"),\n",
    "    (\"REPLACE WITH YOUR TEXT\", \"NEGATIVE\"),\n",
    "    # Add more...\n",
    "]\n",
    "\n",
    "print(\"MY CUSTOM EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, expected in my_evaluation:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    result = sentiment(text)[0]\n",
    "    match = result['label'] == expected\n",
    "    correct += int(match)\n",
    "    total += 1\n",
    "    marker = \"YES\" if match else \"NO <--\"\n",
    "    print(f\"{text[:40]:<42} Expected: {expected:<10} Got: {result['label']:<10} {marker}\")\n",
    "\n",
    "if total > 0:\n",
    "    print(f\"\\nAccuracy: {correct}/{total} = {correct/total:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Model Comparison App\n",
    "\n",
    "In Session 3 you saw Gradio turn one model into a web app. Now let's do something more powerful -- compare all three sentiment models side by side in one interface.\n",
    "\n",
    "This time the app has **one input and three outputs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def compare_three_models(text):\n",
    "    a = model_a(text)[0]\n",
    "    b = model_b(text)[0]\n",
    "    c = model_c(text)[0]\n",
    "    result_a = f\"{a['label']} ({a['score']:.1%})\"\n",
    "    result_b = f\"{b['label']} ({b['score']:.1%})\"\n",
    "    result_c = f\"{c['label']} ({c['score']:.1%})\"\n",
    "    return result_a, result_b, result_c\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=compare_three_models,\n",
    "    inputs=gr.Textbox(label=\"Type any text\", lines=2,\n",
    "                     placeholder=\"Try sarcasm, mixed sentiment, or slang...\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Model A (Movie Reviews)\"),\n",
    "        gr.Textbox(label=\"Model B (Twitter)\"),\n",
    "        gr.Textbox(label=\"Model C (Product Reviews)\"),\n",
    "    ],\n",
    "    title=\"Sentiment Model Showdown\",\n",
    "    description=\"Same text, three different models. See who disagrees!\",\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** This builds on Session 3's single-model Gradio demo. Students now see that Gradio can have multiple outputs. The shareable link lets them challenge friends to find inputs where the models disagree. Stop the demo by clicking the stop button or restarting the runtime when done.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Checklist: Before You Leave\n",
    "\n",
    "- [ ] Understood that confidence scores show certainty, not correctness\n",
    "- [ ] Tested sarcasm and ambiguous text to find model blind spots\n",
    "- [ ] Compared three different sentiment models on the same inputs\n",
    "- [ ] Discussed why training data explains model disagreements\n",
    "- [ ] Understood the spam detector example (high accuracy, useless model)\n",
    "- [ ] Learned the difference between false positives and false negatives\n",
    "- [ ] Built your own evaluation test suite\n",
    "- [ ] Decided which model you would pick for a specific task\n",
    "- [ ] Saved your work (File > Save a copy in Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking Ahead\n",
    "\n",
    "Next session, we will explore what happens when the real world does not match the training data. Models trained on one kind of text can fail spectacularly on another. Understanding these failure modes -- and knowing how to spot them -- is what separates someone who uses AI from someone who truly understands it.\n",
    "\n",
    "See you next session.\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}