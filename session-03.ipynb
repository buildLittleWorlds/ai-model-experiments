{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Data Cleaning and Feature Engineering\n",
    "## When Models Break and How to Fix the Input\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Discover what makes AI models fail, learn a framework for getting AI to write code for you, and use AI-assisted coding to fix broken inputs.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What pipelines did you explore? |\n",
    "| 0:05-0:35 | Part 1: Breaking Models on Purpose |\n",
    "| 0:35-1:05 | Part 2: AI-Assisted Coding (CLEAR Framework) |\n",
    "| 1:05-1:40 | Part 3: Fix the Input |\n",
    "| 1:40-2:00 | On Your Own: Code reading + modification challenges |\n",
    "\n",
    "### Key Vocabulary\n",
    "\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| **Data Quality** | How clean, complete, and reliable the input data is |\n",
    "| **Noise** | Random errors or irrelevant information in data |\n",
    "| **Adversarial Input** | Data deliberately crafted to confuse a model |\n",
    "| **Domain Mismatch** | When input data is different from training data |\n",
    "| **CLEAR Framework** | Context, Language, Explain, Ask, Requirements -- for prompting AI |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review (0:00-0:05)\n",
    "\n",
    "Last session we explored how data gets represented as numbers so that computers can work with it. We saw that representation choices matter -- the same data in different formats enables different computations.\n",
    "\n",
    "Today we flip the question: **What happens when the input data is messy, weird, or deliberately confusing?**\n",
    "\n",
    "Every AI model you have used so far was tested on clean, well-formed text. Real-world inputs are rarely that tidy. Let's find out what happens when things go wrong.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Breaking Models on Purpose (0:05-0:35)\n",
    "\n",
    "In Sessions 1 and 2, we focused on making models work correctly. Today we do the opposite: **we are going to try to break them.**\n",
    "\n",
    "Why? Because understanding failure is how real AI researchers improve systems. Every limitation you discover has a name in machine learning, and knowing those names is the first step toward fixing problems.\n",
    "\n",
    "Let's start by loading a sentiment analysis model and feeding it increasingly difficult inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import\n",
    "!pip install transformers==4.47.1 gradio -q\n",
    "\n",
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the install cell above**, go to **Runtime > Restart runtime**, then continue from the cell below. This ensures all packages load correctly. This is standard practice -- every data scientist does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **FIND A MODEL: Text Classification**\n",
    ">\n",
    "> Before we start breaking models, let's see what's available. The default sentiment model was trained on English movie reviews -- but there are hundreds of text-classification models on Hugging Face, trained on tweets, product reviews, news, toxic comments, and more.\n",
    ">\n",
    "> 1. Go to [huggingface.co/models?pipeline_tag=text-classification&sort=downloads](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads)\n",
    "> 2. Browse the top results. Notice the variety -- sentiment, emotion, spam, toxicity, topic classification.\n",
    "> 3. Pick one model and **read its model card**: What was it trained on? What language? Any known limitations?\n",
    ">\n",
    "> Keep your model in mind -- later in this session you'll build a Gradio app and can swap in any model you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(\"Sentiment model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean Input -- Everything Works\n",
    "\n",
    "Let's start with a perfectly clear, well-written sentence. This is what the model was designed to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean, clear text -- the model's comfort zone\n",
    "clean_text = \"I absolutely loved this movie. The acting was brilliant and the story was captivating.\"\n",
    "result = sentiment(clean_text)[0]\n",
    "print(f\"Text: {clean_text}\")\n",
    "print(f\"Result: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked perfectly. High confidence, correct answer. Now let's see what happens when we start making the input worse.\n",
    "\n",
    "### Step 2: Typos and Misspellings -- Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typos and misspellings\n",
    "noisy_text = \"Ths moive ws realy gret!\"\n",
    "result = sentiment(noisy_text)[0]\n",
    "print(f\"Text: {noisy_text}\")\n",
    "print(f\"Result: {result['label']} (confidence: {result['score']:.1%})\")\n",
    "print()\n",
    "print(\"Compare to the clean version:\")\n",
    "clean_result = sentiment(\"This movie was really great!\")[0]\n",
    "print(f\"Text: This movie was really great!\")\n",
    "print(f\"Result: {clean_result['label']} (confidence: {clean_result['score']:.1%})\")\n",
    "print()\n",
    "print(\"ML term: This is NOISE -- random errors that degrade the signal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Sarcasm -- Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarcasm -- says one thing, means the opposite\n",
    "sarcastic_texts = [\n",
    "    \"Oh great, another Monday. Just what I needed.\",\n",
    "    \"Wow, what a surprise. The bus is late again.\",\n",
    "    \"Sure, I love waiting in line for two hours. Best day ever.\",\n",
    "]\n",
    "\n",
    "print(\"Sarcasm test:\")\n",
    "print(\"=\" * 60)\n",
    "for text in sarcastic_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Model says: {result['label']} ({result['score']:.1%})\")\n",
    "    print()\n",
    "\n",
    "print(\"ML term: This is AMBIGUITY -- the literal words\")\n",
    "print(\"conflict with the intended meaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Mixed Languages -- Domain Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed languages -- the model was trained on English\n",
    "mixed_texts = [\n",
    "    \"This movie was tres magnifique but also schlecht\",\n",
    "    \"The food was oishii and the service was terrible\",\n",
    "    \"Me encanta this place pero the prices are muy altos\",\n",
    "]\n",
    "\n",
    "print(\"Mixed language test:\")\n",
    "print(\"=\" * 60)\n",
    "for text in mixed_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Model says: {result['label']} ({result['score']:.1%})\")\n",
    "    print()\n",
    "\n",
    "print(\"ML term: This is DOMAIN MISMATCH -- the model was trained\")\n",
    "print(\"on English text, so other languages confuse it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5: Spam vs Ham -- Another Classification Task (Book Enhancement)\n",
    "\n",
    "Spam detection is another text-classification problem. Real systems use this for email, comments, and DMs.\n",
    "\n",
    "We will test normal spam/ham messages first, then adversarial versions where spam is disguised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam classification model from the book\n",
    "spam_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"Delphia/twitter-spam-classifier\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    \"Congratulations! You won a free prize. Click this link now.\",\n",
    "    \"Hi team, reminder: project check-in starts at 3:30 PM.\",\n",
    "    \"URGENT: Your account will be closed unless you verify today.\",\n",
    "    \"Can you send me the notes from math class?\"\n",
    "]\n",
    "\n",
    "print(\"SPAM VS HAM\")\n",
    "print(\"=\" * 60)\n",
    "for msg in messages:\n",
    "    result = spam_classifier(msg)[0]\n",
    "    label = \"SPAM\" if str(result['label']) == \"1\" else \"HAM\"\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {label} ({result['score']:.1%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial spam: same intent, obfuscated wording\n",
    "adversarial_messages = [\n",
    "    \"C0NGRATS!!! cl1ck n0w to claim your g1ft card\",\n",
    "    \"Fr33 prize waiting, tap here ASAP\",\n",
    "    \"Hey, can we meet after school to review chapter 4?\"\n",
    "]\n",
    "\n",
    "print(\"ADVERSARIAL SPAM TEST\")\n",
    "print(\"=\" * 60)\n",
    "for msg in adversarial_messages:\n",
    "    result = spam_classifier(msg)[0]\n",
    "    label = \"SPAM\" if str(result['label']) == \"1\" else \"HAM\"\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {label} ({result['score']:.1%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **READ THE MODEL CARD**\n",
    ">\n",
    "> We just used `Delphia/twitter-spam-classifier` — but should we trust it? Check the source:\n",
    ">\n",
    "> Go to [huggingface.co/Delphia/twitter-spam-classifier](https://huggingface.co/Delphia/twitter-spam-classifier)\n",
    ">\n",
    "> - What data was it trained on?\n",
    "> - How many examples did it learn from?\n",
    "> - Is it designed for tweets, emails, or something else?\n",
    "> - What limitations does it mention?\n",
    ">\n",
    "> Reading model cards is how professionals decide whether to trust a model for their use case. A spam detector trained on Twitter may not catch email spam — that's domain mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to invent one spam message and one normal message. Test both and compare confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student spam test\n",
    "student_message = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "if \"REPLACE\" not in student_message:\n",
    "    result = spam_classifier(student_message)[0]\n",
    "    label = \"SPAM\" if str(result['label']) == \"1\" else \"HAM\"\n",
    "    print(f\"Message: {student_message}\")\n",
    "    print(f\"Prediction: {label} ({result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extreme Length -- Short vs Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very short input\n",
    "short_text = \"ok\"\n",
    "result = sentiment(short_text)[0]\n",
    "print(f\"Short text: '{short_text}'\")\n",
    "print(f\"Model says: {result['label']} ({result['score']:.1%})\")\n",
    "print()\n",
    "\n",
    "# Very long, repetitive input\n",
    "long_text = \"This was good. \" * 50\n",
    "result = sentiment(long_text[:512])[0]  # Models have max length\n",
    "print(f\"Long text: '{long_text[:60]}...' (repeated 50 times)\")\n",
    "print(f\"Model says: {result['label']} ({result['score']:.1%})\")\n",
    "print()\n",
    "print(\"Short inputs give the model very little to work with.\")\n",
    "print(\"Long inputs may get truncated -- the model only sees the first part.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Wrong Task -- Adversarial Input\n",
    "\n",
    "Let's try a completely different kind of attack: asking the model to analyze text that has nothing to do with sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text with no sentiment at all\n",
    "neutral_texts = [\n",
    "    \"The chemical formula for water is H2O.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"There are 24 hours in a day.\",\n",
    "]\n",
    "\n",
    "print(\"Neutral/factual text test:\")\n",
    "print(\"=\" * 60)\n",
    "for text in neutral_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Model says: {result['label']} ({result['score']:.1%})\")\n",
    "    print()\n",
    "\n",
    "print(\"These are factual statements with no sentiment.\")\n",
    "print(\"But the model MUST pick positive or negative -- it has no 'neutral' option.\")\n",
    "print(\"This is a design limitation, not a data problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Question Answering Under Pressure\n",
    "\n",
    "Sentiment is not the only model we can break. Let's try a question-answering model with a question that is NOT in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QA pipeline\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "# Ask something that IS in the context\n",
    "context = \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize.\"\n",
    "question_good = \"What did Marie Curie research?\"\n",
    "result = qa(question=question_good, context=context)\n",
    "print(\"Good question (answer IS in the context):\")\n",
    "print(f\"  Q: {question_good}\")\n",
    "print(f\"  A: {result['answer']} (confidence: {result['score']:.1%})\")\n",
    "print()\n",
    "\n",
    "# Ask something NOT in the context\n",
    "question_bad = \"What is Marie Curie's favorite food?\"\n",
    "result = qa(question=question_bad, context=context)\n",
    "print(\"Bad question (answer is NOT in the context):\")\n",
    "print(f\"  Q: {question_bad}\")\n",
    "print(f\"  A: {result['answer']} (confidence: {result['score']:.1%})\")\n",
    "print()\n",
    "print(\"The model tries to answer even when the answer is not there.\")\n",
    "print(\"Notice the low confidence -- that is the model saying 'I am guessing.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Zero-Shot with Ambiguous Categories\n",
    "\n",
    "Let's try one more model: zero-shot classification with categories that all kind of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zero-shot classifier\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Ambiguous text with overlapping categories\n",
    "ambiguous_text = \"I spent the weekend painting landscapes in the park while listening to music\"\n",
    "categories = [\"art\", \"nature\", \"music\", \"exercise\", \"relaxation\"]\n",
    "\n",
    "result = classifier(ambiguous_text, categories)\n",
    "print(f\"Text: {ambiguous_text}\")\n",
    "print(f\"\\nCategories and scores:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    bar = \"*\" * int(score * 30)\n",
    "    print(f\"  {label:12} {bar} ({score:.1%})\")\n",
    "print()\n",
    "print(\"When multiple categories fit, the model spreads its confidence.\")\n",
    "print(\"This is not a failure -- it is honest uncertainty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students: \"What input do you think would confuse this model? What sentence could you write that would be hard for ANY model to classify?\" Let them suggest adversarial inputs. Type them in the cell below and run it live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student adversarial input -- try to break the model!\n",
    "student_breaker = \"REPLACE WITH STUDENT SUGGESTION -- TRY TO BREAK THE MODEL\"\n",
    "\n",
    "# Try it on sentiment\n",
    "result_sent = sentiment(student_breaker)[0]\n",
    "print(f\"Sentiment says: {result_sent['label']} ({result_sent['score']:.1%})\")\n",
    "\n",
    "# Try it on zero-shot\n",
    "result_zs = classifier(student_breaker, [\"positive\", \"negative\", \"confusing\", \"nonsense\"])\n",
    "print(f\"\\nZero-shot says: {result_zs['labels'][0]} ({result_zs['scores'][0]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Why Models Break\n",
    "\n",
    "Every one of these problems has a name in machine learning:\n",
    "\n",
    "| Problem | ML Term | Example |\n",
    "|---------|---------|---------|\n",
    "| Typos and errors | **Noise** | \"Ths moive ws gret\" |\n",
    "| Sarcasm | **Ambiguity** | \"Oh great, another Monday\" |\n",
    "| Foreign words | **Domain mismatch** | \"tres magnifique\" |\n",
    "| Very short text | **Insufficient signal** | \"ok\" |\n",
    "| No right answer exists | **Design limitation** | Sentiment on factual text |\n",
    "| Deliberately confusing | **Adversarial input** | Student suggestions! |\n",
    "\n",
    "Real AI systems need to handle all of this. The first step is recognizing the problem -- and now you can.\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"Show a model card's 'Limitations' section on Hugging Face. Go to huggingface.co/distilbert-base-uncased-finetuned-sst-2-english and scroll to 'Limitations and bias'. Point out: Every model tells you what it's bad at. Reading this section is a professional skill.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: AI-Assisted Coding -- The CLEAR Framework (0:35-1:05)\n",
    "\n",
    "Now that we know what breaks models, let's learn a skill that real programmers use every day: **getting AI to write code for you.**\n",
    "\n",
    "You do not need to memorize Python syntax. You need to:\n",
    "1. **Know what you want to build**\n",
    "2. **Ask AI to write the code clearly**\n",
    "3. **Understand what the AI wrote**\n",
    "4. **Modify it to do what you want**\n",
    "\n",
    "The key to step 2 is a good prompt. Bad prompts give bad code. Good prompts give code you can actually use.\n",
    "\n",
    "### The CLEAR Framework\n",
    "\n",
    "| Letter | Meaning | Example |\n",
    "|--------|---------|---------|\n",
    "| **C** | Context | \"I'm working in Google Colab with Python\" |\n",
    "| **L** | Language/Libraries | \"Using the transformers library\" |\n",
    "| **E** | Explain the goal | \"I want to analyze homework assignments\" |\n",
    "| **A** | Ask specifically | \"Write a function called homework_analyzer\" |\n",
    "| **R** | Requirements | \"Include comments explaining each step\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Prompt vs Good Prompt\n",
    "\n",
    "**Bad prompt:**\n",
    "> \"Write code that analyzes homework\"\n",
    "\n",
    "**Good CLEAR prompt:**\n",
    "> \"I'm working in Google Colab with Python. Using the Hugging Face transformers library, write a function called `homework_analyzer` that:\n",
    "> - Takes a string of text (a homework answer) as input\n",
    "> - Uses a summarization pipeline to create a 1-sentence summary\n",
    "> - Uses a sentiment pipeline to assess the tone (confident vs uncertain)\n",
    "> - Returns a dictionary with the summary, sentiment label, and confidence score\n",
    "> - Includes comments explaining each step\n",
    "> - Prints a formatted report at the end\"\n",
    "\n",
    "Which prompt do you think will give better results?\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"Open Claude or ChatGPT in a new tab. Show students the CLEAR prompt above. Type it live -- or paste it. Show the response. Copy the generated code into Colab. Run it. Debug together if needed. This entire Part 2 IS the AI demo moment.\"\n",
    "\n",
    "### The Live Exercise\n",
    "\n",
    "The cell below contains a pre-written version of what the AI should generate. If the live demo works, use the AI's code instead. If it has bugs, use this as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def homework_analyzer(text):\n",
    "    \"\"\"Analyze a homework answer: summarize it and assess the tone.\"\"\"\n",
    "\n",
    "    # Step 1: Load the summarization pipeline\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "\n",
    "    # Step 2: Load the sentiment pipeline\n",
    "    sentiment_checker = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    # Step 3: Summarize the homework answer\n",
    "    # min_length and max_length control the summary size\n",
    "    if len(text.split()) > 30:\n",
    "        summary = summarizer(text, max_length=50, min_length=10, do_sample=False)\n",
    "        summary_text = summary[0]['summary_text']\n",
    "    else:\n",
    "        summary_text = text  # Too short to summarize\n",
    "\n",
    "    # Step 4: Check the sentiment (confident vs uncertain tone)\n",
    "    tone = sentiment_checker(text)[0]\n",
    "\n",
    "    # Step 5: Build the result dictionary\n",
    "    result = {\n",
    "        'summary': summary_text,\n",
    "        'tone': tone['label'],\n",
    "        'confidence': round(tone['score'], 3)\n",
    "    }\n",
    "\n",
    "    # Step 6: Print a formatted report\n",
    "    print(\"=\" * 50)\n",
    "    print(\"HOMEWORK ANALYSIS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Original ({len(text.split())} words):\")\n",
    "    print(f\"  {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  {summary_text}\")\n",
    "    print(f\"\\nTone: {tone['label']} ({tone['score']:.1%} confidence)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"homework_analyzer function is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the homework analyzer\n",
    "sample_homework = \"\"\"The water cycle begins when the sun heats water in oceans,\n",
    "lakes, and rivers, causing it to evaporate into the atmosphere. The water vapor\n",
    "rises and cools, forming clouds through condensation. When the clouds become\n",
    "heavy enough, precipitation falls as rain, snow, or hail. The water then flows\n",
    "back into bodies of water through runoff and the cycle begins again. This\n",
    "continuous process is essential for distributing fresh water around the planet.\"\"\"\n",
    "\n",
    "result = homework_analyzer(sample_homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the `homework_analyzer` function into Claude or ChatGPT and ask:\n",
    "> *\"Can you explain what each line does in plain English?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Try Different Inputs\n",
    "\n",
    "Test the analyzer with different kinds of homework answers. What happens with short answers? Confident answers? Uncertain ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH STUDENT SUGGESTION -- try different homework answers\n",
    "student_homework = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "result = homework_analyzer(student_homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fix the Input (1:05-1:25)\n",
    "\n",
    "Now let's connect Parts 1 and 2. In Part 1, we discovered what breaks models. In Part 2, we learned to use AI to write code. Now let's **use AI-assisted coding to fix the broken inputs from Part 1.**\n",
    "\n",
    "### A Simple Text Cleaning Function\n",
    "\n",
    "The function below handles some of the most common input problems: extra whitespace, inconsistent capitalization, and repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean up messy text before sending it to a model.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Fix repeated characters (e.g., \"sooooo\" -> \"soo\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "    # Fix common typos/abbreviations\n",
    "    replacements = {\n",
    "        'u ': 'you ',\n",
    "        'ur ': 'your ',\n",
    "        'r ': 'are ',\n",
    "        'thx': 'thanks',\n",
    "        'pls': 'please',\n",
    "        'bc ': 'because ',\n",
    "        'w/': 'with ',\n",
    "        'w/o': 'without',\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Demo the cleaning\n",
    "messy_examples = [\n",
    "    \"   this   movie    was   soooooo    good   \",\n",
    "    \"u should watch this bc its amazinggggg\",\n",
    "    \"thx for the recommendation pls send more\",\n",
    "]\n",
    "\n",
    "print(\"Text cleaning demo:\")\n",
    "print(\"=\" * 60)\n",
    "for messy in messy_examples:\n",
    "    cleaned = clean_text(messy)\n",
    "    print(f\"Before: '{messy}'\")\n",
    "    print(f\"After:  '{cleaned}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Running Broken Inputs Through Cleaning\n",
    "\n",
    "Let's take some of the inputs that confused the model in Part 1 and see if cleaning helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# The broken inputs from Part 1\n",
    "broken_inputs = [\n",
    "    \"Ths moive ws realy gret!\",\n",
    "    \"   this   movie    was   soooooo    good   \",\n",
    "    \"u should watch this bc its amazinggggg\",\n",
    "]\n",
    "\n",
    "print(\"Before and after cleaning:\")\n",
    "print(\"=\" * 70)\n",
    "for text in broken_inputs:\n",
    "    # Analyze the messy version\n",
    "    messy_result = sentiment(text)[0]\n",
    "\n",
    "    # Clean it, then analyze\n",
    "    cleaned = clean_text(text)\n",
    "    clean_result = sentiment(cleaned)[0]\n",
    "\n",
    "    print(f\"Messy:   '{text}'\")\n",
    "    print(f\"  Model: {messy_result['label']} ({messy_result['score']:.1%})\")\n",
    "    print(f\"Cleaned: '{cleaned}'\")\n",
    "    print(f\"  Model: {clean_result['label']} ({clean_result['score']:.1%})\")\n",
    "    change = clean_result['score'] - messy_result['score']\n",
    "    if abs(change) > 0.001:\n",
    "        direction = \"higher\" if change > 0 else \"lower\"\n",
    "        print(f\"  Confidence change: {abs(change):.1%} {direction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Cleaning Cannot Fix\n",
    "\n",
    "Some problems are not about messy text -- they are about **meaning**. Let's test the limits of cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarcasm -- cleaning does not help here\n",
    "sarcastic = \"Oh great, another Monday. Just what I needed.\"\n",
    "cleaned_sarcastic = clean_text(sarcastic)\n",
    "\n",
    "result_before = sentiment(sarcastic)[0]\n",
    "result_after = sentiment(cleaned_sarcastic)[0]\n",
    "\n",
    "print(\"Sarcasm test:\")\n",
    "print(f\"  Original: '{sarcastic}'\")\n",
    "print(f\"  Cleaned:  '{cleaned_sarcastic}'\")\n",
    "print(f\"  Before cleaning: {result_before['label']} ({result_before['score']:.1%})\")\n",
    "print(f\"  After cleaning:  {result_after['label']} ({result_after['score']:.1%})\")\n",
    "print()\n",
    "print(\"Cleaning did not help. The text was already clean.\")\n",
    "print(\"Sarcasm is a MEANING problem, not a FORMATTING problem.\")\n",
    "print(\"Fixing sarcasm requires a model that understands context and tone --\")\n",
    "print(\"that is a much harder problem than text cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the `clean_text` function into Claude or ChatGPT and ask:\n",
    "> *\"What other text cleaning steps would help a sentiment model? Can you add 3 more improvements?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "**What can cleaning fix?**\n",
    "- Extra whitespace, repeated characters, common abbreviations\n",
    "- Inconsistent formatting\n",
    "\n",
    "**What can cleaning NOT fix?**\n",
    "- Sarcasm and irony (meaning problems)\n",
    "- Domain mismatch (wrong language)\n",
    "- Missing information (too short to analyze)\n",
    "- Design limitations (no neutral option)\n",
    "\n",
    "This distinction -- **formatting problems vs meaning problems** -- is important. Data scientists spend most of their time cleaning data, but they also need to know when cleaning is not enough.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Share Your Model as an App\n",
    "\n",
    "You have been running models in code cells. But what if you wanted to share a sentiment analyzer with someone who does not know Python?\n",
    "\n",
    "**Gradio** turns any Python function into a web app in a few lines of code. When you run the cell below, it creates a link anyone can click -- no coding required on their end.\n",
    "\n",
    "**But first -- which model powers your app?** You can use the default sentiment model, or swap in one you found on the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SWAP SLOT: Choose the model for your Gradio app ──\n",
    "# Option A: Use the default sentiment model (already loaded above)\n",
    "# Option B: Swap in a model you found on the Hub!\n",
    "\n",
    "my_model_id = \"PASTE YOUR MODEL ID HERE\"\n",
    "# Example: \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "# Example: \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "# Uncomment these two lines to use your own model:\n",
    "# from transformers import pipeline\n",
    "# sentiment = pipeline(\"text-classification\", model=my_model_id)\n",
    "\n",
    "# Then run the Gradio cell below -- it will use whichever model\n",
    "# is loaded as `sentiment`\n",
    "print(\"Using default sentiment model. Uncomment above to swap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def check_sentiment(text):\n",
    "    result = sentiment(text)[0]\n",
    "    return f\"{result['label']} ({result['score']:.1%})\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=check_sentiment,\n",
    "    inputs=gr.Textbox(label=\"Type any text\", lines=2,\n",
    "                     placeholder=\"Try sarcasm, slang, or mixed feelings...\"),\n",
    "    outputs=gr.Textbox(label=\"Sentiment\"),\n",
    "    title=\"Sentiment Checker\",\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** The `share=True` flag creates a public URL that works for about 72 hours. Students can text this link to friends or family and let them try the sentiment checker. This is their first \"I built an AI app\" moment. Stop the demo by clicking the stop button or restarting the runtime when done.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Exercise 1: Code Detective\n",
    "\n",
    "Look at this code and answer the questions WITHOUT running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS YET -- just read it!\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def mystery_function(texts):\n",
    "    classifier = pipeline(\"zero-shot-classification\")\n",
    "    categories = [\"urgent\", \"normal\", \"spam\"]\n",
    "\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = classifier(text, categories)\n",
    "        top_category = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        results.append({\n",
    "            'message': text,\n",
    "            'category': top_category,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "\n",
    "    urgent_count = sum(1 for r in results if r['category'] == 'urgent')\n",
    "\n",
    "    return results, urgent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions (answer before running):**\n",
    "\n",
    "1. What does this function do?\n",
    "   - Your answer:\n",
    "\n",
    "2. What type of input does it expect?\n",
    "   - Your answer:\n",
    "\n",
    "3. What does it return?\n",
    "   - Your answer:\n",
    "\n",
    "4. What would be a good name for this function instead of `mystery_function`?\n",
    "   - Your answer:\n",
    "\n",
    "5. What would happen if you gave it an empty list?\n",
    "   - Your answer:\n",
    "\n",
    "**Now run it to check your answers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run and test it\n",
    "from transformers import pipeline\n",
    "\n",
    "def mystery_function(texts):\n",
    "    classifier = pipeline(\"zero-shot-classification\")\n",
    "    categories = [\"urgent\", \"normal\", \"spam\"]\n",
    "\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = classifier(text, categories)\n",
    "        top_category = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        results.append({\n",
    "            'message': text,\n",
    "            'category': top_category,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "\n",
    "    urgent_count = sum(1 for r in results if r['category'] == 'urgent')\n",
    "\n",
    "    return results, urgent_count\n",
    "\n",
    "# Test it\n",
    "test_messages = [\n",
    "    \"URGENT: Your account will be suspended!\",\n",
    "    \"Meeting tomorrow at 3pm.\",\n",
    "    \"You've won a FREE iPhone! Click here!\",\n",
    "    \"Please review the attached document when you have time.\"\n",
    "]\n",
    "\n",
    "results, urgent = mystery_function(test_messages)\n",
    "print(f\"Found {urgent} urgent messages\\n\")\n",
    "for r in results:\n",
    "    print(f\"{r['category'].upper()}: {r['message'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Modify the Code\n",
    "\n",
    "Can you modify the code above to:\n",
    "1. Add a new category: \"promotional\"\n",
    "2. Also count spam messages\n",
    "3. Print a warning if more than half the messages are urgent or spam\n",
    "\n",
    "Try to do this yourself first, then use AI if you get stuck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR MODIFIED VERSION\n",
    "# Try to make the changes yourself!\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def email_sorter(texts):\n",
    "    classifier = pipeline(\"zero-shot-classification\")\n",
    "    # TODO: Add \"promotional\" to categories\n",
    "    categories = [\"urgent\", \"normal\", \"spam\"]\n",
    "\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = classifier(text, categories)\n",
    "        top_category = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        results.append({\n",
    "            'message': text,\n",
    "            'category': top_category,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "\n",
    "    urgent_count = sum(1 for r in results if r['category'] == 'urgent')\n",
    "    # TODO: Count spam too\n",
    "\n",
    "    # TODO: Add warning if more than half are urgent or spam\n",
    "\n",
    "    return results, urgent_count\n",
    "\n",
    "# Test your modified version\n",
    "test_messages = [\n",
    "    \"URGENT: Your account will be suspended!\",\n",
    "    \"Meeting tomorrow at 3pm.\",\n",
    "    \"You've won a FREE iPhone! Click here!\",\n",
    "    \"50% off all items this weekend only!\",\n",
    "    \"Please review the attached document when you have time.\"\n",
    "]\n",
    "\n",
    "results, urgent = email_sorter(test_messages)\n",
    "for r in results:\n",
    "    print(f\"{r['category'].upper()}: {r['message'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Modify the Homework Analyzer\n",
    "\n",
    "Add sentence counting to `homework_analyzer`: report whether the answer is \"too short\" (1-2 sentences), \"good length\" (3-5 sentences), or \"detailed\" (6+ sentences).\n",
    "\n",
    "**Hint:** `text.count('.') + text.count('!') + text.count('?')` gives you sentence count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR MODIFIED homework_analyzer\n",
    "# Add sentence counting and length assessment\n",
    "\n",
    "# Hint: text.count('.') + text.count('!') + text.count('?') gives you sentence count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> If you get stuck on Exercise 3, copy the original `homework_analyzer` function into Claude and ask:\n",
    "> *\"Can you add a sentence counter to this function that reports whether the answer is 'too short' (1-2 sentences), 'good length' (3-5 sentences), or 'detailed' (6+ sentences)?\"*\n",
    ">\n",
    "> Compare what the AI gives you to what you tried on your own. What did you get right? What did you miss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist: Before You Leave\n",
    "\n",
    "- [ ] Tested clean text on the sentiment model and saw high confidence\n",
    "- [ ] Fed typos, sarcasm, mixed languages, and short text to the model\n",
    "- [ ] Tried adversarial inputs (your own suggestions)\n",
    "- [ ] Learned the CLEAR Framework for prompting AI to write code\n",
    "- [ ] Built (or saw) the homework_analyzer function\n",
    "- [ ] Used the clean_text function to fix broken inputs\n",
    "- [ ] Discovered what cleaning can and cannot fix\n",
    "- [ ] Browsed the Hub for text-classification models and read a model card\n",
    "- [ ] Read the mystery_function and answered the questions\n",
    "- [ ] Tried at least one modification challenge\n",
    "\n",
    "**Save your work:** File > Save a copy in Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Ahead\n",
    "\n",
    "Today you learned what makes models fail and how to fix some of those problems through data cleaning. You also learned the CLEAR Framework for getting AI to write code for you.\n",
    "\n",
    "Next session, we go deeper into how models actually learn. You have used pre-trained models that someone else built. But what does \"trained\" really mean? How did those models go from knowing nothing to being useful?\n",
    "\n",
    "Session 4 answers that question: **supervised learning** -- teaching a model by showing it labeled examples.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Models can be broken by **noise**, **ambiguity**, **domain mismatch**, and **adversarial input**\n",
    "2. Every model has limitations -- read the model card to find them\n",
    "3. The **CLEAR Framework** helps you write better prompts for AI-assisted coding\n",
    "4. **Text cleaning** fixes formatting problems but not meaning problems\n",
    "5. Understanding failure is the first step toward building better systems\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}