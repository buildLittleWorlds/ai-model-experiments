{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: Model Training and Parameters\n",
    "## The Dials You Control\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Discover that AI models have settings YOU can change -- called hyperparameters -- and see how those settings dramatically affect what the model produces.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What supervised learning examples did you find? |\n",
    "| 0:05-0:35 | Part 1: Hyperparameters -- The Dials You Control |\n",
    "| 0:35-1:00 | Part 2: Parameters in Other Models |\n",
    "| 1:00-1:40 | Part 3: Systematic Parameter Sweep |\n",
    "| 1:40-2:00 | On Your Own: Extended parameter experiments |\n",
    "\n",
    "### Key Vocabulary\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| Parameters | The billions of patterns a model learned during training (you can't change these) |\n",
    "| Hyperparameters | Settings YOU control that affect how the model behaves |\n",
    "| Temperature | Controls how creative vs. predictable text generation is |\n",
    "| Top-p | Limits the model to only the most likely next words |\n",
    "| Beam Search | A method where the model considers multiple options before committing |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What Supervised Learning Examples Did You Find? (0:00-0:05)\n",
    "\n",
    "Last session we saw how machines learn from labeled examples -- training data goes in, patterns come out, and the model makes predictions on data it has never seen.\n",
    "\n",
    "Today we flip the question around. Instead of asking \"how does the model learn?\", we ask: **\"what can I control about how the model behaves?\"**\n",
    "\n",
    "The answer: more than you might think.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.47.1 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart Your Runtime\n",
    "\n",
    "After installing packages, you need to restart the runtime so Python can find them.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "After restarting, come back here and continue running the cells below. You do NOT need to re-run the install cell -- the packages are already installed. Just start from the next code cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Hyperparameters -- The Dials You Control (0:05-0:35)\n",
    "\n",
    "Here is the key idea for today:\n",
    "\n",
    "- The model has **BILLIONS of learned parameters** -- patterns it picked up during training. You cannot see or change those. They are baked into the model.\n",
    "- But you CAN control settings that affect how the model **uses** those parameters. These settings are called **hyperparameters**.\n",
    "- **Temperature** is a hyperparameter. It controls how creative vs. predictable the model is.\n",
    "\n",
    "Think of it like a radio. The radio station (the model's learned knowledge) is fixed. But you control the volume, the bass, the treble. Those are the dials. Today we learn what the dials do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **FIND A MODEL: Text Generation**\n",
    ">\n",
    "> The model we'll use today (`distilgpt2`) is a small text generator. But there are many text-generation models on the Hub, each with different strengths.\n",
    ">\n",
    "> 1. Go to [huggingface.co/models?pipeline_tag=text-generation&sort=downloads](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)\n",
    "> 2. Browse the top results. Notice the range -- tiny models like `distilgpt2`, medium models, and huge ones that need GPUs.\n",
    "> 3. Pick a model that says **\"cpu\"** or does NOT list a minimum GPU requirement. **Read its model card**: What was it trained on? What language? Any known limitations?\n",
    "> 4. Copy the model ID -- you can try it in the swap slot below.\n",
    ">\n",
    "> **Important:** Many text-generation models require a GPU. Stick with small models (under 500M parameters) to run on free Colab CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Text Generator\n",
    "\n",
    "We will use `distilgpt2`, a small but real text generation model. It predicts what comes next in a sentence, one word at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SWAP SLOT: Text Generation Model ──\n",
    "# Default: distilgpt2 (small, CPU-friendly, trained on web text)\n",
    "# To use a model you found on the Hub, paste its ID below:\n",
    "\n",
    "my_model = \"PASTE YOUR MODEL ID HERE\"\n",
    "# Example: \"distilgpt2\"  (82M params, CPU ok)\n",
    "# Example: \"sshleifer/tiny-gpt2\"  (even smaller, fast)\n",
    "\n",
    "# Uncomment these two lines to use your own model:\n",
    "# generator = pipeline(\"text-generation\", model=my_model)\n",
    "# print(f\"Loaded custom model: {my_model}\")\n",
    "\n",
    "# Default: use distilgpt2\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(\"Generator loaded: distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Experiment\n",
    "\n",
    "Temperature controls how \"adventurous\" the model is when picking the next word.\n",
    "\n",
    "- **Low temperature (0.3):** The model almost always picks the most probable next word. Safe, predictable, sometimes boring.\n",
    "- **Medium temperature (0.7-1.0):** A balance between predictable and surprising.\n",
    "- **High temperature (1.3):** The model picks less likely words more often. Creative, surprising, sometimes nonsensical.\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Before running the cell, ask students: \"If the prompt is 'The robot opened the door and saw,' what do you think it will say at temperature 0.3 vs 1.3? More random? Weirder? Let them guess, then run it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The robot opened the door and saw\"\n",
    "\n",
    "print(\"TEMPERATURE EXPERIMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0, 1.3]:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    creativity = \"Conservative\" if temp < 0.5 else \"Balanced\" if temp < 0.9 else \"Creative\" if temp < 1.1 else \"Wild\"\n",
    "    print(f\"\\nTemp {temp} ({creativity}):\")\n",
    "    print(f\"  {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the pattern. Low temperature text reads like a textbook -- correct but dull. High temperature text takes risks -- sometimes brilliant, sometimes gibberish. This is the fundamental tradeoff in generative AI: **safety vs. creativity**.\n",
    "\n",
    "### Top-P Experiment\n",
    "\n",
    "Top-p (also called \"nucleus sampling\") is a different kind of dial. Instead of adjusting how adventurous the model is, it limits **which words the model is even allowed to consider**.\n",
    "\n",
    "- **Top-p = 0.5:** Only consider words that make up the top 50% of probability. Very restricted vocabulary.\n",
    "- **Top-p = 0.95:** Consider words that make up the top 95% of probability. Almost everything is on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time in a magical forest,\"\n",
    "\n",
    "print(\"TOP-P EXPERIMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for top_p in [0.5, 0.8, 0.95]:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=60,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    print(f\"\\nTop-P {top_p}:\")\n",
    "    print(f\"  {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Prompt\n",
    "\n",
    "Let's test temperature and top-p with a prompt from the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to suggest a prompt. Type it into the cell below, then run the cell to see how temperature changes the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_prompt = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "print(f\"Prompt: {student_prompt}\\n\")\n",
    "for temp in [0.3, 0.7, 1.0, 1.3]:\n",
    "    result = generator(\n",
    "        student_prompt,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    creativity = \"Conservative\" if temp < 0.5 else \"Balanced\" if temp < 0.9 else \"Creative\" if temp < 1.1 else \"Wild\"\n",
    "    print(f\"Temp {temp} ({creativity}):\")\n",
    "    print(f\"  {result[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **READ THE MODEL CARD**\n",
    ">\n",
    "> We've been using `distilgpt2` -- but what was it actually trained on? Check the source:\n",
    ">\n",
    "> Go to [huggingface.co/distilgpt2](https://huggingface.co/distilgpt2)\n",
    ">\n",
    "> - What dataset was it trained on? (Hint: look for \"OpenWebTextCorpus\")\n",
    "> - How many parameters does it have?\n",
    "> - What does the \"Limitations and bias\" section say?\n",
    "> - How does it compare to the full GPT-2 model?\n",
    ">\n",
    "> Model cards document the hyperparameter choices the model creators made during training -- the same kind of choices you're experimenting with today.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parameters in Other Models (0:35-1:00)\n",
    "\n",
    "Temperature and top-p are hyperparameters for text generation. But **every model has its own dials**. Let's explore a few different model types and see what you can control in each one.\n",
    "\n",
    "### Summarization: Controlling Length and Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** For each experiment in Part 2, ask students to predict the result before running the cell. \"What do you think will happen if we set max_length to 20 vs 100?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "print(\"Summarizer loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_length: How Long Should the Summary Be?\n",
    "\n",
    "The `max_length` hyperparameter tells the model the maximum number of tokens (roughly words) it can use in the summary. Watch how the same article gets compressed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Artificial intelligence has made remarkable progress in recent years. \\\n",
    "Large language models can now write essays, translate between languages, and even \\\n",
    "generate code. Computer vision systems can identify objects in photos with superhuman \\\n",
    "accuracy. Self-driving cars use AI to navigate roads, and recommendation systems \\\n",
    "powered by AI decide what shows you see on Netflix and what posts appear in your \\\n",
    "social media feeds. Despite these advances, AI still struggles with common sense \\\n",
    "reasoning, understanding context the way humans do, and explaining its own decisions. \\\n",
    "Researchers are working on making AI systems more transparent and trustworthy.\"\"\"\n",
    "\n",
    "print(\"SUMMARIZATION: max_length EXPERIMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original text: {len(text.split())} words\\n\")\n",
    "\n",
    "for length in [20, 40, 60, 100]:\n",
    "    summary = summarizer(text, max_length=length, min_length=10)\n",
    "    print(f\"max_length={length}:\")\n",
    "    print(f\"  {summary[0]['summary_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model has to make harder decisions when the length is shorter. At max_length=20, it can only keep the most essential idea. At max_length=100, it can include nuance and detail. This is a real design decision -- do you want a tweet-length summary or a paragraph?\n",
    "\n",
    "#### num_beams: Greedy vs. Beam Search\n",
    "\n",
    "When the model generates a summary, it picks words one at a time. With **greedy search** (num_beams=1), it always picks the single most likely next word. With **beam search** (num_beams=4), it considers 4 different paths at each step and picks the best overall sequence.\n",
    "\n",
    "Think of it like navigating a maze. Greedy search always turns toward the exit at every intersection. Beam search explores a few paths at once and picks the one that actually gets you out fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUMMARIZATION: BEAM SEARCH EXPERIMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for beams in [1, 4]:\n",
    "    summary = summarizer(text, max_length=50, min_length=15, num_beams=beams)\n",
    "    label = \"Greedy (first guess)\" if beams == 1 else \"Beam search (considers options)\"\n",
    "    print(f\"\\n{label} (num_beams={beams}):\")\n",
    "    print(f\"  {summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the summarization code into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"What is beam search and why does it sometimes produce better summaries? Explain it like I'm in high school.\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "### Zero-Shot Classification: Labels as Parameters\n",
    "\n",
    "Here is a model where the \"hyperparameter\" is not a number -- it is the **set of categories** you give it. The same text can be classified completely differently depending on what labels you offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Visualization (Book Enhancement)\n",
    "\n",
    "Before a model can reason over text, it breaks text into **tokens**.\n",
    "\n",
    "- Word-level idea: split by words\n",
    "- Subword-level idea: split rare words into pieces\n",
    "- Character-level idea: split into letters/symbols\n",
    "\n",
    "Most modern transformer models use subword tokenization.\n",
    "\n",
    "> **INSTRUCTOR NOTE:** Use the `unhappiness` example below to show how one word can become multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "token_examples = [\n",
    "    \"I love cats\",\n",
    "    \"What is unhappiness?\",\n",
    "    \"AI models read tokens, not raw sentences.\"\n",
    "]\n",
    "\n",
    "print(\"TOKENIZATION EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for text in token_examples:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs (first 10): {token_ids[:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "print(\"Classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The new iPhone has an amazing camera but the battery life is disappointing\"\n",
    "\n",
    "print(\"ZERO-SHOT: CHANGING LABELS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "label_sets = [\n",
    "    [\"positive\", \"negative\"],\n",
    "    [\"tech review\", \"personal opinion\", \"news\"],\n",
    "    [\"praise\", \"criticism\", \"mixed\"]\n",
    "]\n",
    "\n",
    "for labels in label_sets:\n",
    "    result = classifier(text, labels)\n",
    "    print(f\"Labels: {labels}\")\n",
    "    for lbl, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  {lbl}: {score:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same text, completely different analysis -- just by changing the labels. The labels you choose are a design decision, and they dramatically affect what insights you get.\n",
    "\n",
    "### Question Answering: Context as a Parameter\n",
    "\n",
    "In question-answering, the model extracts answers from a passage you provide. The passage itself is a kind of parameter -- change the passage, change the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "question = \"What is the main challenge?\"\n",
    "\n",
    "contexts = [\n",
    "    \"Climate change is the main challenge facing humanity. Rising temperatures threaten ecosystems worldwide.\",\n",
    "    \"The main challenge in education is keeping students engaged. Traditional lectures often fail to hold attention.\",\n",
    "    \"For software developers, the main challenge is managing complexity. As systems grow, bugs multiply.\"\n",
    "]\n",
    "\n",
    "print(\"QA: SAME QUESTION, DIFFERENT CONTEXTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "for i, context in enumerate(contexts, 1):\n",
    "    result = qa(question=question, context=context)\n",
    "    print(f\"Context {i}: {context[:60]}...\")\n",
    "    print(f\"  Answer: {result['answer']} (confidence: {result['score']:.1%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Go to huggingface.co and navigate to a model card (e.g., distilgpt2). Show the \"Usage\" section. Point out that model creators often suggest parameter ranges -- what temperature works well, what max_length to use, etc. This is where professionals look for guidance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Systematic Parameter Sweep (1:00-1:25)\n",
    "\n",
    "So far we have been testing a few values by hand. But real researchers do this **systematically** -- they sweep through many values and record the results in a table. This lets you see patterns instead of guessing.\n",
    "\n",
    "### Temperature Sweep\n",
    "\n",
    "Let's run the same prompt through 7 different temperature values and compare the results side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "print(f\"{'Temp':<6} {'Output':<70}\")\n",
    "print(\"=\" * 76)\n",
    "\n",
    "for temp in [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3]:\n",
    "    result = generator(prompt, max_length=40, do_sample=True, temperature=temp)\n",
    "    output = result[0]['generated_text'][len(prompt):].strip()[:65]\n",
    "    print(f\"{temp:<6} {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Table\n",
    "\n",
    "For each temperature in the sweep, rate the output on these dimensions:\n",
    "\n",
    "| Temp | Coherent? (1-5) | Surprising? (1-5) | Grammatical? (1-5) | Useful? (1-5) |\n",
    "|------|-----|------|------|------|\n",
    "| 0.1 | | | | |\n",
    "| 0.3 | | | | |\n",
    "| 0.5 | | | | |\n",
    "| 0.7 | | | | |\n",
    "| 0.9 | | | | |\n",
    "| 1.1 | | | | |\n",
    "| 1.3 | | | | |\n",
    "\n",
    "### The Big Takeaway\n",
    "\n",
    "There is no \"right\" temperature. It depends on what you want:\n",
    "- Writing a factual summary? **Low temperature** (0.2-0.4).\n",
    "- Brainstorming creative ideas? **High temperature** (0.9-1.2).\n",
    "- Chatbot conversation? **Medium temperature** (0.6-0.8).\n",
    "\n",
    "This is a **design choice**, not a technical one. You are the designer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Sweep\n",
    "\n",
    "Now let's do the same sweep with a prompt from the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students for a prompt. Type it below. Then run the sweep to see how temperature affects their specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_sweep_prompt = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "print(f\"Prompt: {student_sweep_prompt}\")\n",
    "print()\n",
    "print(f\"{'Temp':<6} {'Output':<70}\")\n",
    "print(\"=\" * 76)\n",
    "\n",
    "for temp in [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3]:\n",
    "    result = generator(student_sweep_prompt, max_length=40, do_sample=True, temperature=temp)\n",
    "    output = result[0]['generated_text'][len(student_sweep_prompt):].strip()[:65]\n",
    "    print(f\"{temp:<6} {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy your completed observation table into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"I ran a temperature sweep on a text generation model. Here are my ratings for each temperature. What temperature would you recommend for [describe your use case], and why?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Experiment 1: Your Own Temperature Sweep\n",
    "\n",
    "Pick a prompt that interests you -- a story opening, a question, a description. Run it through the full temperature range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = \"REPLACE WITH YOUR OWN PROMPT\"\n",
    "\n",
    "print(f\"Prompt: {my_prompt}\")\n",
    "print()\n",
    "print(f\"{'Temp':<6} {'Output':<70}\")\n",
    "print(\"=\" * 76)\n",
    "\n",
    "for temp in [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3]:\n",
    "    result = generator(my_prompt, max_length=40, do_sample=True, temperature=temp)\n",
    "    output = result[0]['generated_text'][len(my_prompt):].strip()[:65]\n",
    "    print(f\"{temp:<6} {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Temperature + Top-P Together\n",
    "\n",
    "What happens when you combine the two dials? Try different combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt_2 = \"REPLACE WITH YOUR OWN PROMPT\"\n",
    "\n",
    "print(f\"Prompt: {my_prompt_2}\")\n",
    "print()\n",
    "print(f\"{'Temp':<6} {'Top-P':<6} {'Output':<60}\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.2]:\n",
    "    for top_p in [0.5, 0.9]:\n",
    "        result = generator(\n",
    "            my_prompt_2,\n",
    "            max_length=40,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        output = result[0]['generated_text'][len(my_prompt_2):].strip()[:55]\n",
    "        print(f\"{temp:<6} {top_p:<6} {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Summarization Length vs. Quality\n",
    "\n",
    "Try summarizing a paragraph you care about (a news article, a textbook passage, something you wrote) at different max_length values. When does the summary lose important information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"REPLACE WITH A PARAGRAPH YOU WANT TO SUMMARIZE\"\n",
    "\n",
    "print(\"YOUR SUMMARIZATION EXPERIMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original: {len(my_text.split())} words\\n\")\n",
    "\n",
    "for length in [15, 30, 50, 80]:\n",
    "    summary = summarizer(my_text, max_length=length, min_length=5)\n",
    "    print(f\"max_length={length}:\")\n",
    "    print(f\"  {summary[0]['summary_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Checklist: Before You Leave\n",
    "\n",
    "- [ ] Understood the difference between parameters (learned) and hyperparameters (set by you)\n",
    "- [ ] Ran temperature experiments and saw the creativity-predictability tradeoff\n",
    "- [ ] Ran top-p experiments\n",
    "- [ ] Explored hyperparameters in summarization (max_length, num_beams)\n",
    "- [ ] Saw how zero-shot labels change classification results\n",
    "- [ ] Completed a systematic parameter sweep\n",
    "- [ ] Filled in the observation table\n",
    "- [ ] Browsed text-generation models on the Hub and read a model card\n",
    "- [ ] Tried your own prompt experiments\n",
    "- [ ] Saved your work (File > Save a copy in Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking Ahead\n",
    "\n",
    "Next session, we will learn how to **evaluate** models -- not just \"does it work?\" but \"how well does it work, and how do we measure that?\" We will compare multiple models head-to-head, learn about confidence scores, and discover why a model can be 95% accurate and still completely useless.\n",
    "\n",
    "See you next session.\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}