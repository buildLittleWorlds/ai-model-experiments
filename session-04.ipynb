{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Introduction to Supervised Learning\n",
    "## How Models Learn From Labeled Examples\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Understand what \"training\" really means by exploring supervised learning -- the process of teaching a model by showing it labeled examples.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What did you break? What did you fix? |\n",
    "| 0:05-0:30 | Part 1: What Does \"Trained on Labeled Data\" Mean? |\n",
    "| 0:30-1:05 | Part 2: Three Types of Supervised Tasks |\n",
    "| 1:05-1:40 | Part 3: How Would You Label This Data? |\n",
    "| 1:40-2:00 | On Your Own: Explore track notebooks + model cards |\n",
    "\n",
    "### Key Vocabulary\n",
    "\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| **Supervised Learning** | Teaching a model by showing it labeled examples |\n",
    "| **Training Data** | The examples a model learned from |\n",
    "| **Labels** | The correct answers attached to training examples |\n",
    "| **Pre-trained Model** | A model someone else already trained, ready to use |\n",
    "| **Model Card** | Documentation describing how a model was built and what it's good at |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review (0:00-0:05)\n",
    "\n",
    "Last session, we broke models on purpose and learned how to fix inputs through data cleaning. We saw that models can be confused by noise, sarcasm, domain mismatch, and adversarial inputs.\n",
    "\n",
    "But here is the deeper question: **why do models get confused at all?**\n",
    "\n",
    "The answer comes down to **training**. Every model you have used was trained on specific data. When the input matches what it trained on, it works well. When the input is different from the training data, it struggles.\n",
    "\n",
    "Today we unpack what \"training\" actually means.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What Does \"Trained on Labeled Data\" Mean? (0:05-0:30)\n",
    "\n",
    "Every model you have used in this course was **trained** by someone before you used it. Training means: someone collected thousands (or millions) of examples, attached correct answers to each one, and fed them into a learning algorithm.\n",
    "\n",
    "The model studied those examples and learned to find patterns. Then, when you give it a new input it has never seen before, it uses those patterns to make a prediction.\n",
    "\n",
    "This process -- learning from examples with correct answers -- is called **supervised learning**. The \"supervision\" is the labels: the correct answers that guide the model's learning.\n",
    "\n",
    "### Real Models, Real Data\n",
    "\n",
    "Here is what some of the models you have used actually trained on:\n",
    "\n",
    "| Model | Training Data | How Many Examples |\n",
    "|-------|--------------|-------------------|\n",
    "| Sentiment analysis | Movie reviews labeled POSITIVE or NEGATIVE | ~67,000 reviews |\n",
    "| Emotion detection | Tweets labeled with 7 emotions | ~58,000 tweets |\n",
    "| Image classification | Photos labeled with 1,000 categories | ~14 million images |\n",
    "| Text generation (GPT-2) | Web pages, books, articles | ~8 million web pages |\n",
    "\n",
    "Every one of these models exists because someone collected data, labeled it, and trained a model on it. No data, no model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import\n",
    "!pip install transformers==4.47.1 -q\n",
    "\n",
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After running the install cell above**, go to **Runtime > Restart runtime**, then continue from the cell below. This ensures all packages load correctly. This is standard practice -- every data scientist does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the Connection: Model to Data\n",
    "\n",
    "Let's load an emotion classifier and trace its predictions back to its training data.\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"Open huggingface.co/j-hartmann/emotion-english-distilroberta-base in a browser tab. Show the model card. Click through to the training data link. Show students: this model exists because someone labeled 58,000 tweets with emotions. The model card tells you everything about how a model was built.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the emotion classifier\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    top_k=None  # Get all emotion scores\n",
    ")\n",
    "\n",
    "# Run it on a clear example\n",
    "text = \"I can't believe I won the contest! This is the best day ever!\"\n",
    "emotions = emotion_classifier(text)[0]\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print()\n",
    "print(\"Emotions detected:\")\n",
    "for e in sorted(emotions, key=lambda x: x['score'], reverse=True):\n",
    "    bar = \"*\" * int(e['score'] * 20)\n",
    "    print(f\"  {e['label']:12} {bar} ({e['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicted \"joy\" with high confidence. But **where did it learn that \"I can't believe I won!\" is joy?**\n",
    "\n",
    "From 58,000 labeled tweets. Someone read each tweet and tagged it: \"this one is joy,\" \"this one is anger,\" \"this one is sadness.\" The model studied those examples and learned:\n",
    "- Exclamation marks + winning/achieving words --> probably joy\n",
    "- Insults + frustration words --> probably anger\n",
    "- Loss + missing words --> probably sadness\n",
    "\n",
    "It did not memorize rules. It discovered **patterns** in the labeled data.\n",
    "\n",
    "### The Supervised Learning Recipe\n",
    "\n",
    "Every supervised learning project follows the same recipe:\n",
    "\n",
    "```\n",
    "1. COLLECT data          (gather examples)\n",
    "2. LABEL the data        (attach correct answers)\n",
    "3. TRAIN the model       (let it study the examples)\n",
    "4. TEST the model        (check its predictions on new data)\n",
    "5. DEPLOY the model      (let people use it)\n",
    "```\n",
    "\n",
    "You have been using models at Step 5. Today we look at Steps 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Labels?\n",
    "\n",
    "Labels are the \"correct answers\" attached to training examples. The type of label determines the type of task.\n",
    "\n",
    "| Task | Input | Label | Example |\n",
    "|------|-------|-------|---------|\n",
    "| Sentiment analysis | \"Great movie!\" | POSITIVE | Positive review |\n",
    "| Emotion detection | \"I'm furious\" | anger | Angry tweet |\n",
    "| Image classification | Photo of a dog | \"golden retriever\" | ImageNet label |\n",
    "| Spam detection | \"You won $1M!\" | SPAM | Email label |\n",
    "\n",
    "Without labels, a model cannot learn. The labels are the \"supervision\" in supervised learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Three Types of Supervised Tasks (0:30-1:05)\n",
    "\n",
    "Now let's see supervised learning in action across three different types of tasks. Each uses the same principle -- learning from labeled examples -- but with different kinds of labels.\n",
    "\n",
    "### Task 1: Emotion Classification (Multi-Class Text Classification)\n",
    "\n",
    "This model learned from ~58,000 tweets, each labeled with one of 7 emotions.\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"For each model, ask students to suggest inputs. Type their suggestions into the student_text variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **READ THE MODEL CARD**\n",
    ">\n",
    "> The emotion model we just loaded is `j-hartmann/emotion-english-distilroberta-base`. Before testing it further, check its documentation:\n",
    ">\n",
    "> Go to [huggingface.co/j-hartmann/emotion-english-distilroberta-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base)\n",
    ">\n",
    "> - What 7 emotions can it detect?\n",
    "> - What dataset was it trained on? How many examples?\n",
    "> - Is it designed for tweets, formal writing, or both?\n",
    "> - What limitations does the card mention?\n",
    ">\n",
    "> Understanding a model's training data helps you predict where it will succeed and where it will struggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion classification -- trained on 58,000 labeled tweets\n",
    "student_text = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "emotions = emotion_classifier(student_text)[0]\n",
    "\n",
    "print(f\"Text: {student_text}\")\n",
    "print()\n",
    "print(\"Emotion scores:\")\n",
    "for e in sorted(emotions, key=lambda x: x['score'], reverse=True):\n",
    "    bar = \"*\" * int(e['score'] * 30)\n",
    "    print(f\"  {e['label']:12} {bar} ({e['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Copy the emotion classifier code into Claude or ChatGPT and ask:\n",
    "> *\"This model was trained on 58,000 tweets. What does that mean for how it handles formal writing vs. social media text?\"*\n",
    ">\n",
    "> This is how real researchers think about model limitations -- by connecting performance to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Runtime Restart\n",
    "\n",
    "Before loading the image model, restart your runtime to free up memory.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "Then re-run the install cell and continue from the cell below.\n",
    "\n",
    "This clears the text models from memory so you have room for image models. Professional data scientists do this all the time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Image Classification\n",
    "\n",
    "This model learned from ~14 million images, each labeled with one of 1,000 categories (from the ImageNet dataset).\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"Show the model card for google/vit-base-patch16-224 on Hugging Face. Point out: 14 million images, 1,000 categories. Someone had to label all of those.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **FIND A MODEL: Image Classification**\n",
    ">\n",
    "> The default image classifier (`google/vit-base-patch16-224`) was trained on ImageNet -- 14 million photos in 1,000 categories. But there are many other image classifiers on the Hub, trained on different datasets for different purposes.\n",
    ">\n",
    "> 1. Go to [huggingface.co/models?pipeline_tag=image-classification&sort=downloads](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads)\n",
    "> 2. Browse the top results. You'll see models for general objects, food, animals, medical images, and more.\n",
    "> 3. Pick one model and **read its model card**: What images was it trained on? How many categories? Any known limitations?\n",
    "> 4. Copy the model ID -- you'll use it in the swap slot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Helper function to load images from URLs\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "# ── SWAP SLOT: Image Classification Model ──\n",
    "# Default: google/vit-base-patch16-224 (trained on ImageNet, 1000 categories)\n",
    "# To use a model you found on the Hub, paste its ID below:\n",
    "\n",
    "my_image_model = \"PASTE YOUR MODEL ID HERE\"\n",
    "# Example: \"google/vit-base-patch16-224\"  (ImageNet, general objects)\n",
    "# Example: \"nateraw/food\"  (food classification)\n",
    "# Example: \"dima806/bird_species_image_classification\"  (bird species)\n",
    "\n",
    "# Uncomment the line below to use your own model:\n",
    "# image_classifier = pipeline(\"image-classification\", model=my_image_model)\n",
    "\n",
    "# Default: use the standard ImageNet model\n",
    "image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "print(\"Image classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\"\n",
    "\n",
    "image = load_image_from_url(image_url)\n",
    "display(image.resize((300, 300)))\n",
    "\n",
    "results = image_classifier(image)\n",
    "print(\"\\nTop predictions:\")\n",
    "for r in results[:5]:\n",
    "    bar = \"*\" * int(r['score'] * 30)\n",
    "    print(f\"  {r['label']:30} {bar} ({r['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicted \"Labrador retriever\" with high confidence. Where did it learn this?\n",
    "\n",
    "From the ImageNet dataset. Researchers at Stanford collected 14 million images and organized them into 1,000 categories. Each image was labeled by a human. The model studied these labeled examples and learned to recognize visual patterns: floppy ears + golden fur + specific body shape = Labrador retriever.\n",
    "\n",
    "### Try More Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different images\n",
    "test_images = {\n",
    "    \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
    "    \"pizza\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Eq_it-na_pizza-margherita_sep2005_sml.jpg/1200px-Eq_it-na_pizza-margherita_sep2005_sml.jpg\",\n",
    "}\n",
    "\n",
    "for name, url in test_images.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    try:\n",
    "        image = load_image_from_url(url)\n",
    "        results = image_classifier(image)\n",
    "        for r in results[:3]:\n",
    "            print(f\"  {r['label']:30} ({r['score']:.1%})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2B: Object Detection (Book Enhancement)\n",
    "\n",
    "Image classification answers: **\"What is in this image?\"**\n",
    "\n",
    "Object detection answers: **\"What is in this image, and where is each object?\"**\n",
    "\n",
    "This is a major upgrade because we get bounding boxes, not just labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **FIND A MODEL: Object Detection**\n",
    ">\n",
    "> Object detection models identify AND locate objects in images. The default (`facebook/detr-resnet-50`) was trained on COCO -- 80 everyday object categories.\n",
    ">\n",
    "> 1. Go to [huggingface.co/models?pipeline_tag=object-detection&sort=downloads](https://huggingface.co/models?pipeline_tag=object-detection&sort=downloads)\n",
    "> 2. Browse the models. Some specialize in faces, vehicles, or specific domains.\n",
    "> 3. **Read the model card** for one: What objects can it detect? How many categories? What dataset was it trained on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Object detection model from the book\n",
    "object_detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "def load_image_for_detection(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "print(\"Object detector loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image with multiple objects\n",
    "object_image_url = \"https://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "object_image = load_image_for_detection(object_image_url)\n",
    "\n",
    "display(object_image.resize((420, 320)))\n",
    "\n",
    "detections = object_detector(object_image)\n",
    "print(\"Detected objects:\")\n",
    "for obj in detections[:10]:\n",
    "    print(f\"- {obj['label']} ({obj['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes on the image\n",
    "annotated = object_image.copy()\n",
    "draw = ImageDraw.Draw(annotated)\n",
    "\n",
    "for obj in detections[:10]:\n",
    "    box = obj['box']\n",
    "    coords = [box['xmin'], box['ymin'], box['xmax'], box['ymax']]\n",
    "    label_text = f\"{obj['label']} {obj['score']:.2f}\"\n",
    "    draw.rectangle(coords, outline=\"red\", width=3)\n",
    "    draw.text((coords[0], max(0, coords[1] - 12)), label_text, fill=\"red\")\n",
    "\n",
    "display(annotated.resize((420, 320)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to suggest a new image URL with multiple objects (street, classroom, sports scene). Run detection and inspect mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student object-detection test\n",
    "student_object_image_url = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "if \"REPLACE\" not in student_object_image_url:\n",
    "    student_image = load_image_for_detection(student_object_image_url)\n",
    "    student_detections = object_detector(student_image)\n",
    "    display(student_image.resize((420, 320)))\n",
    "    print(\"Top detections:\")\n",
    "    for obj in student_detections[:8]:\n",
    "        print(f\"- {obj['label']} ({obj['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Runtime Restart\n",
    "\n",
    "Before loading the text generation model, restart your runtime to free up memory.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "Then re-run the install cell and continue from the cell below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Text Generation\n",
    "\n",
    "Text generation models learned to **predict the next word** from billions of examples. The \"label\" is the word that actually came next in the training text.\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"Show the model card for distilgpt2 on Hugging Face. Point out: this model was trained on a large corpus of text to predict the next word. Every word it generates is based on patterns it learned from real text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load text generation -- trained by predicting the next word\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Generate a continuation\n",
    "prompt = \"The most important thing about learning AI is\"\n",
    "\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_length=60,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"Continuation {i}:\")\n",
    "    print(f\"  {r['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model's \"labels\" were different from the others. Instead of categories, the label was always: **what word actually came next?**\n",
    "\n",
    "The model read billions of sentences and for each position, it tried to guess the next word. When it guessed wrong, it adjusted its internal numbers. After seeing enough examples, it got good at predicting what comes next.\n",
    "\n",
    "That is why it can generate text that sounds plausible -- it learned the statistical patterns of how words follow each other.\n",
    "\n",
    "### Comparing All Three Tasks\n",
    "\n",
    "All three models use supervised learning. The difference is the type of label:\n",
    "\n",
    "| Task | Input | Label Type | Training Size |\n",
    "|------|-------|-----------|---------------|\n",
    "| Emotion detection | Text | One of 7 emotion categories | ~58,000 tweets |\n",
    "| Image classification | Image | One of 1,000 object categories | ~14 million images |\n",
    "| Text generation | Text | The next word in the sequence | Billions of words |\n",
    "\n",
    "The principle is the same: **show the model labeled examples, and it learns patterns.**\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"After all three demos, ask students: Which model was most accurate? Most surprising? Which would be hardest to train -- meaning, which would need the most labeled data?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: How Would You Label This Data? (1:05-1:25)\n",
    "\n",
    "Now let's flip the perspective. Instead of using models, let's think about **creating training data.** This exercise will show you why labeling is harder than it sounds.\n",
    "\n",
    "### The Labeling Challenge\n",
    "\n",
    "Below are real examples that need labels. For each one, decide what the correct label should be. Write your answer, then discuss with the class.\n",
    "\n",
    "> **INSTRUCTOR NOTE:** \"Read each example aloud. Ask students to call out their labels. Write them on screen. The point is that students will DISAGREE -- and that disagreement is a real problem in ML.\"\n",
    "\n",
    "### Round 1: Emotion Labeling\n",
    "\n",
    "For each tweet, choose one emotion: **joy, anger, sadness, fear, surprise, disgust, neutral**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeling challenge -- what emotion is each tweet?\n",
    "tweets_to_label = [\n",
    "    \"I can't even right now\",\n",
    "    \"Just found out school is cancelled tomorrow!!!\",\n",
    "    \"My dog ate my homework. No really, he actually did.\",\n",
    "    \"Why do people even bother trying anymore\",\n",
    "    \"lol this is fine everything is fine\",\n",
    "]\n",
    "\n",
    "print(\"EMOTION LABELING CHALLENGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Choose one: joy, anger, sadness, fear, surprise, disgust, neutral\")\n",
    "print()\n",
    "for i, tweet in enumerate(tweets_to_label, 1):\n",
    "    print(f'  {i}. \"{tweet}\"')\n",
    "    print(f\"     Your label: _______________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2: Sentiment Labeling\n",
    "\n",
    "For each review, choose: **positive or negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_label = [\n",
    "    \"It was fine, I guess.\",\n",
    "    \"Not the worst thing I've ever seen.\",\n",
    "    \"I expected more, but it was okay.\",\n",
    "    \"My mom loved it, but I thought it was boring.\",\n",
    "    \"3 out of 5 stars.\",\n",
    "]\n",
    "\n",
    "print(\"SENTIMENT LABELING CHALLENGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Choose one: positive or negative\")\n",
    "print()\n",
    "for i, review in enumerate(reviews_to_label, 1):\n",
    "    print(f'  {i}. \"{review}\"')\n",
    "    print(f\"     Your label: _______________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3: Image Category Labeling\n",
    "\n",
    "Imagine you are labeling photos. What category would you assign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_descriptions = [\n",
    "    \"A photo of latte art in a coffee cup\",\n",
    "    \"A person jogging through a forest\",\n",
    "    \"A cat sitting on a laptop keyboard\",\n",
    "    \"A sunset over a city skyline\",\n",
    "    \"A child painting at an easel\",\n",
    "]\n",
    "\n",
    "possible_categories = [\"food\", \"sports\", \"animals\", \"nature\", \"art\", \"technology\", \"people\"]\n",
    "\n",
    "print(\"IMAGE CATEGORY LABELING CHALLENGE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Choose from: {', '.join(possible_categories)}\")\n",
    "print()\n",
    "for i, desc in enumerate(image_descriptions, 1):\n",
    "    print(f\"  {i}. {desc}\")\n",
    "    print(f\"     Your label: _______________\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Why Labeling Is Hard\n",
    "\n",
    "**Think about what just happened:**\n",
    "\n",
    "1. **Did everyone in the class agree on every label?** Probably not. \"I can't even right now\" -- is that anger? Sadness? Frustration is not even one of the options.\n",
    "\n",
    "2. **Some examples fit multiple categories.** A photo of latte art is food AND art. A person jogging in a forest is sports AND nature. Forcing one label onto multi-category data loses information.\n",
    "\n",
    "3. **Context matters.** \"It was fine, I guess\" could be positive (it was acceptable) or negative (disappointed). Without knowing who said it and why, you cannot be sure.\n",
    "\n",
    "4. **Scale matters.** You just labeled about 15 examples. Imagine doing this 58,000 times for the emotion model. Now imagine doing it 14 million times for the image model. Errors are inevitable at that scale.\n",
    "\n",
    "**The key insight:** When humans disagree about labels, the model learns from that disagreement. If 60% of labelers call a tweet \"anger\" and 40% call it \"sadness,\" the model will learn that this kind of tweet is ambiguous -- which is actually the correct answer.\n",
    "\n",
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Ask Claude or ChatGPT:\n",
    "> *\"If humans disagree about whether a tweet is sad or angry, what happens when an AI model trains on those labels?\"*\n",
    ">\n",
    "> The answer connects directly to what we discovered in Session 3 about model confidence scores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Exercise 1: Explore Model Cards\n",
    "\n",
    "Go to [huggingface.co/models](https://huggingface.co/models) and find a model that interests you. Read its model card and answer:\n",
    "\n",
    "1. **What was it trained on?** (What dataset? How many examples?)\n",
    "2. **What task does it solve?** (Classification? Generation? Something else?)\n",
    "3. **What are its limitations?** (Every good model card has a limitations section)\n",
    "4. **Who built it?** (A company? A university? An individual?)\n",
    "\n",
    "**Write your findings here:**\n",
    "\n",
    "Model name:\n",
    "\n",
    "Trained on:\n",
    "\n",
    "Task:\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Built by:\n",
    "\n",
    "### Exercise 2: Design Your Own Training Dataset\n",
    "\n",
    "Think of a task you would like a model to solve. Then design the training data:\n",
    "\n",
    "1. **What is the task?** (Example: \"Classify student questions as homework-help, conceptual, or off-topic\")\n",
    "2. **What would the input look like?** (Example: A student's question text)\n",
    "3. **What would the labels be?** (Example: homework-help, conceptual, off-topic)\n",
    "4. **How many examples would you need?** (Hundreds? Thousands? Millions?)\n",
    "5. **What would be hard to label?** (What edge cases would cause disagreement?)\n",
    "\n",
    "**Your design:**\n",
    "\n",
    "Task:\n",
    "\n",
    "Input:\n",
    "\n",
    "Labels:\n",
    "\n",
    "How many examples:\n",
    "\n",
    "Hard to label:\n",
    "\n",
    "### Exercise 3: Track Preview\n",
    "\n",
    "Want to go deeper? Here is what each specialization track covers in upcoming sessions:\n",
    "\n",
    "| Track | Focus | You Will Build |\n",
    "|-------|-------|----------------|\n",
    "| **Text & Language** | Emotion detection, classification, summarization | Content analyzer, writing feedback tool |\n",
    "| **Images & Vision** | Image classification, captioning, detection | Photo organizer, visual search tool |\n",
    "| **Creative AI** | Text generation, style control, multi-modal | Story generator, creative writing assistant |\n",
    "\n",
    "All three tracks build on supervised learning. The models in each track were trained on different data with different labels, but the principle is identical: **learn from labeled examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist: Before You Leave\n",
    "\n",
    "- [ ] Understand that every model was trained on labeled data\n",
    "- [ ] Can explain the supervised learning recipe (collect, label, train, test, deploy)\n",
    "- [ ] Ran the emotion classifier and traced its predictions to training data\n",
    "- [ ] Read the emotion model's model card on Hugging Face\n",
    "- [ ] Ran the image classifier (default or one you found on the Hub)\n",
    "- [ ] Browsed image-classification models on the Hub\n",
    "- [ ] Ran the text generator and understand it learned to predict the next word\n",
    "- [ ] Tried the labeling challenge and discovered that humans disagree\n",
    "- [ ] Can explain why labeling disagreement affects model performance\n",
    "- [ ] Read at least one model card on Hugging Face\n",
    "\n",
    "**Save your work:** File > Save a copy in Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Ahead\n",
    "\n",
    "Today you learned that supervised learning is about learning from labeled examples. Every model you use was trained this way: someone collected data, labeled it, and trained a model to find patterns.\n",
    "\n",
    "But we skipped a critical question: **how does the training actually work?** When we say a model \"studies\" examples and \"adjusts\" itself, what is really happening mathematically?\n",
    "\n",
    "Next session, we open the black box further. You will learn about **training parameters** -- the knobs you can turn that control how a model learns. You will train your own model and see how changing these settings affects the result.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Supervised learning** means teaching a model by showing it labeled examples\n",
    "2. Every model you have used was trained on thousands to millions of labeled examples\n",
    "3. The **type of label** determines the type of task (emotions, categories, next word)\n",
    "4. **Labeling is harder than it sounds** -- humans disagree, and that affects models\n",
    "5. **Model cards** document how a model was built, trained, and where it struggles\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}