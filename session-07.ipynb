{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7: Overfitting and Generalization\n",
    "## When Models Leave Home\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Discover that AI models are shaped by their training data -- and when the real world looks different from that data, models can fail in surprising ways. Learn to spot domain shift, understand overfitting, and match the right model to the right data.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What did model comparison reveal? |\n",
    "| 0:05-0:35 | Part 1: Domain Shift -- When Models Leave Home |\n",
    "| 0:35-0:55 | Part 2: Memorization vs. Learning |\n",
    "| 0:55-1:40 | Part 3: Matching Models to Data |\n",
    "| 1:40-2:00 | On Your Own: Test models on your own text domains |\n",
    "\n",
    "### Key Vocabulary\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| Domain Shift | When input data differs from training data |\n",
    "| Overfitting | Too good at training data, too bad at new data |\n",
    "| Generalization | Model works on data it hasn't seen before |\n",
    "| Training Domain | The type of data a model learned from |\n",
    "| Memorization | Learning quirks instead of patterns |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What Did Model Comparison Reveal? (0:00-0:05)\n",
    "\n",
    "Last session we learned that **different models give different answers for the same input** -- and that a model can be 95% accurate while being completely useless. We learned about confidence scores, false positives, and false negatives.\n",
    "\n",
    "Today we ask: **why do models fail?** Not just \"they get it wrong sometimes\" -- but what specifically causes a model to break down?\n",
    "\n",
    "The answer is surprisingly simple: **models fail when the real world doesn't look like their training data.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install transformers==4.47.1 -q"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart Your Runtime\n",
    "\n",
    "After installing packages, you need to restart the runtime so Python can find them.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "After restarting, come back here and continue running the cells below. You do NOT need to re-run the install cell -- the packages are already installed. Just start from the next code cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Domain Shift -- When Models Leave Home (0:05-0:35)\n",
    "\n",
    "Every model was trained on a specific kind of data. Movie reviews. Tweets. News articles. Wikipedia. When you give it data that looks **different** from what it trained on, it can fail -- even if the task is exactly the same. This is called **domain shift**.\n",
    "\n",
    "Think of it like this: if you learned to drive in a flat desert, you might be great at desert roads. But put you on a mountain switchback in the rain, and suddenly your \"driving skill\" doesn't transfer. The skill is the same -- driving -- but the **domain** changed.\n",
    "\n",
    "### Load the Default Sentiment Model\n",
    "\n",
    "The default sentiment model was trained on **movie reviews** (the SST-2 dataset). It knows what \"positive\" and \"negative\" look like in movie review language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(\"Default sentiment model loaded!\")\n",
    "print(\"Training data: SST-2 (movie reviews)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Case: Movie Review Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = [\n",
    "    \"The acting was phenomenal and the plot kept me hooked.\",\n",
    "    \"Terrible screenplay with wooden performances throughout.\",\n",
    "    \"A masterpiece of modern cinema that will stand the test of time.\"\n",
    "]\n",
    "\n",
    "print(\"MOVIE REVIEWS (the model's home territory)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for text in movie_reviews:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprises. The model is confident and correct on movie reviews because that is exactly what it was trained on. This is the model's **home territory**.\n",
    "\n",
    "Now let's take it somewhere unfamiliar.\n",
    "\n",
    "### Domain Shift: Tweets and Slang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Before running the next cell, ask students: \"This model was trained on formal movie reviews. What do you think will happen when we give it tweets and slang? Will it still work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_slang = [\n",
    "    \"ngl that movie slapped fr fr\",\n",
    "    \"mid tbh, wouldn't recommend\",\n",
    "    \"bestie this show is giving everything\",\n",
    "    \"that test was bussin no cap\",\n",
    "    \"lowkey obsessed with this album rn\"\n",
    "]\n",
    "\n",
    "print(\"TWEETS AND SLANG (leaving home territory)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for text in tweets_and_slang:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happened. The model might have gotten some right, but look at the **confidence scores** and check whether the predictions actually match what a human would say. Words like \"slapped,\" \"bussin,\" and \"giving\" have positive meanings in slang, but the model may not know that -- because slang was not in its training data.\n",
    "\n",
    "### Domain Shift: Formal and Technical Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_texts = [\n",
    "    \"The patient presents with acute symptoms requiring immediate intervention.\",\n",
    "    \"Quarterly earnings exceeded analyst expectations by a significant margin.\",\n",
    "    \"The defendant's counsel filed a motion to dismiss on procedural grounds.\",\n",
    "    \"The compound exhibited promising results in preliminary trials.\"\n",
    "]\n",
    "\n",
    "print(\"FORMAL / TECHNICAL TEXT\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for text in formal_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical, legal, financial, and scientific text all have their own vocabulary and conventions. A movie review model has no training on this kind of language. It is **guessing** based on whatever patterns it can find -- and those guesses may be wrong even when the confidence is high.\n",
    "\n",
    "### The Sarcasm Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcastic_texts = [\n",
    "    \"What a FANTASTIC use of my Saturday.\",\n",
    "    \"Oh wonderful, another group project. Just what I needed.\",\n",
    "    \"Sure, because that worked so well last time.\",\n",
    "    \"I just love sitting in traffic for two hours. Best part of my day.\"\n",
    "]\n",
    "\n",
    "print(\"SARCASM (the model's worst enemy)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for text in sarcastic_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Prediction: {result['label']} (confidence: {result['score']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarcasm is almost impossible for this model. Why? Because sarcasm uses **positive words to express negative feelings**. The model sees \"fantastic,\" \"wonderful,\" \"love,\" and \"best\" and confidently predicts POSITIVE. But a human immediately recognizes the tone.\n",
    "\n",
    "This is not a bug in the model. It is a **limitation of how it was trained**. The training data (movie reviews) does not contain much sarcasm, so the model never learned to handle it.\n",
    "\n",
    "### Student Test: Your Own Domain Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to suggest slang or informal text from their own lives. Type it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_text = \"REPLACE WITH STUDENT SUGGESTION\"\n",
    "\n",
    "result = sentiment(student_text)[0]\n",
    "print(f\"Text: {student_text}\")\n",
    "print(f\"Prediction: {result['label']} (confidence: {result['score']:.1%})\")\n",
    "print()\n",
    "print(\"Does this match what a human would say? If not, why might the model be wrong?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Paste a slang sentence and its sentiment result into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"This model said NEGATIVE for a sentence that's actually positive. The model was trained on movie reviews. Can you explain why it might have gotten this wrong?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Memorization vs. Learning (0:35-0:55)\n",
    "\n",
    "If you memorized every answer on a practice test, you would ace the practice test -- but bomb the real test. Models do the same thing. They can memorize quirks of their training data instead of learning general patterns. This is called **overfitting**.\n",
    "\n",
    "A model that **generalizes** well has learned the underlying patterns. A model that has **overfit** has memorized specific examples.\n",
    "\n",
    "### Two Models, Two Training Domains\n",
    "\n",
    "Let's load a second sentiment model -- one trained specifically on **tweets**. Then we will compare both models on different kinds of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Default (trained on movie reviews - SST-2)\n",
    "model_reviews = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Model B: Twitter-specific (trained on tweets)\n",
    "model_twitter = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "print(\"Two models loaded!\")\n",
    "print(\"Model A: distilbert-base (movie reviews)\")\n",
    "print(\"Model B: twitter-roberta (tweets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Before running the comparison, ask: \"Which model do you think will handle tweets better? Which will handle formal text better? Why?\"\n",
    "\n",
    "### Head-to-Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets â€” Twitter model's home territory\n",
    "tweets = [\n",
    "    \"ngl that movie slapped fr fr\",\n",
    "    \"mid tbh, wouldn't recommend\",\n",
    "    \"this show is giving everything omg\",\n",
    "    \"ratio + L + didn't ask\",\n",
    "    \"W take honestly\"\n",
    "]\n",
    "\n",
    "# Formal text â€” Review model's home territory\n",
    "formal = [\n",
    "    \"The acting was phenomenal and the plot kept me hooked.\",\n",
    "    \"A disappointing sequel that fails to capture the original's charm.\",\n",
    "    \"The quarterly report indicates strong revenue growth across all sectors.\",\n",
    "]\n",
    "\n",
    "print(\"TWEETS (Twitter model's home territory)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Text':<40} {'Reviews Model':<18} {'Twitter Model':<18}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for text in tweets:\n",
    "    a = model_reviews(text)[0]\n",
    "    b = model_twitter(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    print(f\"{text[:38]:<40} {a_str:<18} {b_str:<18}\")\n",
    "\n",
    "print()\n",
    "print(\"FORMAL TEXT (Reviews model's home territory)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Text':<40} {'Reviews Model':<18} {'Twitter Model':<18}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for text in formal:\n",
    "    a = model_reviews(text)[0]\n",
    "    b = model_twitter(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    print(f\"{text[:38]:<40} {a_str:<18} {b_str:<18}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the pattern. The Twitter model handles tweets better because it was **trained on tweets**. The reviews model handles formal text better because it was **trained on reviews**. Neither model is \"better\" -- they are each better at their own domain.\n",
    "\n",
    "This is the difference between memorization and learning:\n",
    "- A model that **memorized** movie review patterns will ace movie reviews but fail on tweets.\n",
    "- A model that truly **learned** sentiment would work on any text. No current model does this perfectly.\n",
    "\n",
    "### Why This Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Open the model cards for both models on Hugging Face. Show the \"Training Data\" sections. Ask: \"Given what each was trained on, why does the Twitter model handle slang better?\"\n",
    ">\n",
    "> - Default model: [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "> - Twitter model: [cardiffnlp/twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Matching Models to Data (0:55-1:25)\n",
    "\n",
    "The skill is not finding the \"best model.\" The skill is **matching the right model to the right data.**\n",
    "\n",
    "Here are five real scenarios. For each one, think about which model would be a better fit -- and then we will test your prediction.\n",
    "\n",
    "### The Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {\n",
    "    \"Customer support tickets (formal)\": [\n",
    "        \"The product arrived damaged and I need a replacement immediately.\",\n",
    "        \"Your software crashes every time I try to export to PDF.\",\n",
    "        \"Thank you for the quick resolution to my billing issue.\"\n",
    "    ],\n",
    "    \"TikTok comments (informal, slang)\": [\n",
    "        \"this is so fire i can't even\",\n",
    "        \"not the tutorial i needed but the tutorial i deserved lol\",\n",
    "        \"bestie you ate this up\"\n",
    "    ],\n",
    "    \"News article excerpts (journalistic)\": [\n",
    "        \"The legislation passed with bipartisan support after months of negotiation.\",\n",
    "        \"Critics argue the policy will disproportionately affect low-income families.\",\n",
    "        \"Markets rallied on the announcement, with stocks closing at record highs.\"\n",
    "    ],\n",
    "    \"Friend group chat (very informal)\": [\n",
    "        \"bruh that was so extra lmaooo\",\n",
    "        \"im literally dead rn ðŸ’€\",\n",
    "        \"ugh monday again somebody save me\"\n",
    "    ],\n",
    "    \"Job application cover letters (formal)\": [\n",
    "        \"I am writing to express my enthusiastic interest in the position.\",\n",
    "        \"My experience in project management has prepared me well for this role.\",\n",
    "        \"I am confident I can contribute meaningfully to your team.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"MODEL MATCHING EXERCISE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Scenario':<45} {'Reviews Model':<15} {'Twitter Model':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for scenario, texts in scenarios.items():\n",
    "    print(f\"\\n{scenario}\")\n",
    "    for text in texts:\n",
    "        a = model_reviews(text)[0]\n",
    "        b = model_twitter(text)[0]\n",
    "        a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "        b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "        print(f\"  {text[:41]:<43} {a_str:<15} {b_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Look at the results and think about these questions:\n",
    "\n",
    "1. **Which model did better on formal text?** Why?\n",
    "2. **Which model did better on slang and informal text?** Why?\n",
    "3. **Were there any cases where BOTH models failed?** What kind of text was it?\n",
    "4. **If you were building an app to analyze TikTok comments, which model would you pick?**\n",
    "5. **If you were building an app to analyze customer feedback emails, which would you pick?**\n",
    "\n",
    "The answer to \"which model is better?\" is always: **\"Better at what?\"**\n",
    "\n",
    "### Student Challenge: Pick Your Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Have each student pick a domain they care about -- their text messages, Discord messages, school emails, social media comments. Type in 3-4 examples and see which model handles their domain better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_domain = \"My Domain: REPLACE WITH DESCRIPTION\"\n",
    "\n",
    "student_examples = [\n",
    "    \"REPLACE WITH EXAMPLE 1\",\n",
    "    \"REPLACE WITH EXAMPLE 2\",\n",
    "    \"REPLACE WITH EXAMPLE 3\",\n",
    "]\n",
    "\n",
    "print(f\"{student_domain}\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Text':<40} {'Reviews Model':<15} {'Twitter Model':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for text in student_examples:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    a = model_reviews(text)[0]\n",
    "    b = model_twitter(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    print(f\"{text[:38]:<40} {a_str:<15} {b_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> After the model matching exercise, paste your results into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"I tested two sentiment models on different types of text. One was trained on movie reviews, the other on tweets. Here are the results: [paste results]. Which model would you recommend for analyzing [your use case], and why?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Experiment 1: Find Your Own Domain Shift\n",
    "\n",
    "Find text from your own life -- group chats, school emails, social media posts, game chat -- and run it through both models. Document which model handles your data best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_texts = [\n",
    "    \"REPLACE WITH TEXT FROM YOUR LIFE 1\",\n",
    "    \"REPLACE WITH TEXT FROM YOUR LIFE 2\",\n",
    "    \"REPLACE WITH TEXT FROM YOUR LIFE 3\",\n",
    "    \"REPLACE WITH TEXT FROM YOUR LIFE 4\",\n",
    "    \"REPLACE WITH TEXT FROM YOUR LIFE 5\",\n",
    "]\n",
    "\n",
    "print(\"MY DOMAIN SHIFT EXPERIMENT\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Text':<40} {'Reviews Model':<15} {'Twitter Model':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for text in my_texts:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    a = model_reviews(text)[0]\n",
    "    b = model_twitter(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    print(f\"{text[:38]:<40} {a_str:<15} {b_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: The Sarcasm Challenge\n",
    "\n",
    "Write 5 sarcastic sentences and run them through both models. Can either model detect sarcasm reliably?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm_test = [\n",
    "    \"REPLACE WITH SARCASTIC SENTENCE 1\",\n",
    "    \"REPLACE WITH SARCASTIC SENTENCE 2\",\n",
    "    \"REPLACE WITH SARCASTIC SENTENCE 3\",\n",
    "    \"REPLACE WITH SARCASTIC SENTENCE 4\",\n",
    "    \"REPLACE WITH SARCASTIC SENTENCE 5\",\n",
    "]\n",
    "\n",
    "print(\"SARCASM CHALLENGE\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Text':<40} {'Reviews Model':<15} {'Twitter Model':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for text in sarcasm_test:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    a = model_reviews(text)[0]\n",
    "    b = model_twitter(text)[0]\n",
    "    a_str = f\"{a['label'][:3]} {a['score']:.0%}\"\n",
    "    b_str = f\"{b['label'][:3]} {b['score']:.0%}\"\n",
    "    print(f\"{text[:38]:<40} {a_str:<15} {b_str:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Model Recommendation Table\n",
    "\n",
    "Based on everything you tested today, fill in this table:\n",
    "\n",
    "| Data Type | Better Model | Why? |\n",
    "|-----------|-------------|------|\n",
    "| Movie reviews | | |\n",
    "| Tweets and social media | | |\n",
    "| Customer emails | | |\n",
    "| Sarcastic text | | |\n",
    "| News articles | | |\n",
    "| Your own domain: _______ | | |\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist: Before You Leave\n",
    "\n",
    "- [ ] Tested the default model on text outside its training domain\n",
    "- [ ] Saw how slang, sarcasm, and formal text cause different failures\n",
    "- [ ] Compared two models trained on different data\n",
    "- [ ] Understood that neither model is \"better\" -- they fit different domains\n",
    "- [ ] Tested models on 5 real-world scenarios\n",
    "- [ ] Found text from your own life and identified the best model for it\n",
    "- [ ] Saved your work (File > Save a copy in Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking Ahead\n",
    "\n",
    "Next session, we tackle one of the most important topics in AI: **bias**. Models learn from data, data comes from the real world, and the real world has biases. We will find those biases in real models, explore what happens when models are confident but wrong, and discuss who gets hurt when AI systems are unfair.\n",
    "\n",
    "See you next session.\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}