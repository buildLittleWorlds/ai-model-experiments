{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8: Bias, Variance, and Uncertainty\n",
    "## When AI Reflects the World's Problems\n",
    "\n",
    "**Session Length:** 2 hours\n",
    "\n",
    "**Today's Mission:** Investigate how biases in training data lead to biases in AI models, explore what happens when models are confident but wrong, and discuss the real-world consequences of biased AI systems.\n",
    "\n",
    "### Session Outline\n",
    "| Time | Activity |\n",
    "|------|----------|\n",
    "| 0:00-0:05 | Review: What domain shift patterns did you find? |\n",
    "| 0:05-0:35 | Part 1: Finding Bias in Models |\n",
    "| 0:35-0:55 | Part 2: Uncertainty -- When Models Are Confident But Wrong |\n",
    "| 0:55-1:40 | Part 3: Discussion -- Who Gets Hurt? |\n",
    "| 1:40-2:00 | On Your Own: Design your own bias test |\n",
    "\n",
    "### Key Vocabulary\n",
    "| Term | Definition |\n",
    "|------|-----------|\n",
    "| Bias | Systematic patterns in model mistakes |\n",
    "| Training Data Bias | When the data used to train a model doesn't represent everyone equally |\n",
    "| Uncertainty | When the model isn't sure about its answer |\n",
    "| Deterministic | Same input always gives same output |\n",
    "| Stochastic | Same input can give different outputs |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: What Domain Shift Patterns Did You Find? (0:00-0:05)\n",
    "\n",
    "Last session we learned that models fail when the real world doesn't match their training data. A movie review model struggles with tweets. A tweet model struggles with formal text. Neither is \"better\" -- they are each fitted to a different domain.\n",
    "\n",
    "Today we go deeper. We are not just asking \"does the model fail?\" -- we are asking **\"does the model fail unfairly?\"**\n",
    "\n",
    "This is the most important session in the course. The concepts we explore today are what separate someone who uses AI from someone who understands AI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to install the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.47.1 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart Your Runtime\n",
    "\n",
    "After installing packages, you need to restart the runtime so Python can find them.\n",
    "\n",
    "**Go to: Runtime > Restart runtime**\n",
    "\n",
    "After restarting, come back here and continue running the cells below. You do NOT need to re-run the install cell -- the packages are already installed. Just start from the next code cell.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Finding Bias in Models (0:05-0:35)\n",
    "\n",
    "Models learn from data. Data comes from the real world. The real world has biases. So models have biases.\n",
    "\n",
    "This is not about anyone being intentionally unfair. It is about **patterns in the data** that create blind spots. If a model was trained mostly on text written by one demographic, it will understand that demographic's language better. If the training data associated certain jobs with certain genders, the model will learn those associations too.\n",
    "\n",
    "Our job today: **find the bias and measure it.**\n",
    "\n",
    "### Sentiment Analysis: Paired Sentence Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** This is sensitive material. Frame it carefully: \"We're investigating whether training data contained patterns that lead to unequal treatment. This is what real AI researchers and ethics teams do at companies like Google, Microsoft, and OpenAI. Finding bias is the first step to fixing it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(\"Sentiment model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paired sentence test is one of the simplest bias detection methods. We create two sentences that are identical except for one word -- a name, a job title, a demographic marker. If the model treats them differently, that difference comes from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired sentences: same structure, different names/roles\n",
    "paired_tests = [\n",
    "    (\"The doctor was professional and competent.\",\n",
    "     \"The nurse was professional and competent.\"),\n",
    "\n",
    "    (\"James is applying for the management position.\",\n",
    "     \"Lakisha is applying for the management position.\"),\n",
    "\n",
    "    (\"The CEO announced record profits this quarter.\",\n",
    "     \"The receptionist announced record profits this quarter.\"),\n",
    "\n",
    "    (\"He is a brilliant scientist with many publications.\",\n",
    "     \"She is a brilliant scientist with many publications.\"),\n",
    "\n",
    "    (\"The engineer solved the complex technical problem.\",\n",
    "     \"The teacher solved the complex technical problem.\"),\n",
    "]\n",
    "\n",
    "print(\"PAIRED SENTENCE BIAS TEST: SENTIMENT\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for sentence_a, sentence_b in paired_tests:\n",
    "    result_a = sentiment(sentence_a)[0]\n",
    "    result_b = sentiment(sentence_b)[0]\n",
    "\n",
    "    diff = abs(result_a['score'] - result_b['score'])\n",
    "    flag = \" <-- DIFFERENCE\" if diff > 0.05 else \"\"\n",
    "\n",
    "    print(f\"\\nA: {sentence_a}\")\n",
    "    print(f\"   {result_a['label']} ({result_a['score']:.1%})\")\n",
    "    print(f\"B: {sentence_b}\")\n",
    "    print(f\"   {result_b['label']} ({result_b['score']:.1%})\")\n",
    "    if flag:\n",
    "        print(f\"   Gap: {diff:.1%}{flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results carefully. These sentences say the **exact same thing** -- only a name or job title changed. Any difference in the sentiment score comes from **patterns the model learned during training**.\n",
    "\n",
    "Even small differences matter. In a system that processes millions of resumes, a 2% difference in sentiment score could mean thousands of people get ranked differently based on nothing but their name.\n",
    "\n",
    "### Zero-Shot Classification: Association Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **FIND A MODEL: Zero-Shot Classification**\n",
    ">\n",
    "> The zero-shot classifier below can test whether models associate certain people with certain roles. There are several zero-shot models on the Hub, each with different strengths.\n",
    ">\n",
    "> 1. Go to [huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads)\n",
    "> 2. Browse the top results. Notice how some support multiple languages.\n",
    "> 3. Pick a model and **read its model card**: What was it trained on? What languages does it support? Does it mention anything about bias?\n",
    "> 4. Copy the model ID -- you'll use it in the swap slot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "print(\"Zero-shot classifier loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Does the model associate certain people with certain roles?\n",
    "test_sentences = [\n",
    "    \"The person is a skilled professional.\",\n",
    "    \"The young man is a skilled professional.\",\n",
    "    \"The young woman is a skilled professional.\",\n",
    "    \"The elderly person is a skilled professional.\",\n",
    "]\n",
    "\n",
    "categories = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"manager\", \"assistant\"]\n",
    "\n",
    "print(\"ZERO-SHOT BIAS TEST: ROLE ASSOCIATIONS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for text in test_sentences:\n",
    "    result = classifier(text, categories)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  Top 3 predictions:\")\n",
    "    for label, score in zip(result['labels'][:3], result['scores'][:3]):\n",
    "        print(f\"    {label}: {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the predictions change when we changed \"person\" to \"man\" or \"woman\"? If so, the model has learned **associations** between demographics and roles from its training data. These associations reflect real-world biases that exist in the text the model was trained on.\n",
    "\n",
    "### Student Test: Your Own Paired Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students to suggest their own paired sentences. What biases do they want to test for? Age? Names from different cultures? Different occupations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students design their own bias test\n",
    "student_sentence_a = \"REPLACE WITH SENTENCE A\"\n",
    "student_sentence_b = \"REPLACE WITH SENTENCE B (change only one word)\"\n",
    "\n",
    "result_a = sentiment(student_sentence_a)[0]\n",
    "result_b = sentiment(student_sentence_b)[0]\n",
    "\n",
    "print(\"YOUR BIAS TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"A: {student_sentence_a}\")\n",
    "print(f\"   {result_a['label']} ({result_a['score']:.1%})\")\n",
    "print(f\"B: {student_sentence_b}\")\n",
    "print(f\"   {result_b['label']} ({result_b['score']:.1%})\")\n",
    "\n",
    "diff = abs(result_a['score'] - result_b['score'])\n",
    "print(f\"\\nDifference: {diff:.1%}\")\n",
    "if diff > 0.05:\n",
    "    print(\"Notable difference detected! What might explain this?\")\n",
    "else:\n",
    "    print(\"Scores are similar. Try other word swaps to probe further.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ASK AI ABOUT THIS**\n",
    ">\n",
    "> Paste the paired-sentence results into Claude or ChatGPT and ask:\n",
    ">\n",
    "> *\"We got different sentiment scores for these sentences that only differ by name. What might explain this? What does this tell us about the training data?\"*\n",
    ">\n",
    "> This is how real programmers learn -- by asking questions about code they encounter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Uncertainty -- When Models Are Confident But Wrong (0:35-0:55)\n",
    "\n",
    "In Session 6 we learned that confidence scores do not mean correctness. Now let's go further: some models are **deterministic** (same input always gives the same output) and some are **stochastic** (same input can give different outputs).\n",
    "\n",
    "### Confident on Ambiguous Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliberately neutral/ambiguous sentences\n",
    "ambiguous_texts = [\n",
    "    \"The meeting was held on Tuesday.\",\n",
    "    \"She walked to the store.\",\n",
    "    \"The report was submitted.\",\n",
    "    \"They completed the project on time.\",\n",
    "    \"The weather is expected to change.\"\n",
    "]\n",
    "\n",
    "print(\"CONFIDENCE ON NEUTRAL TEXT\")\n",
    "print(\"=\" * 55)\n",
    "print(\"(These sentences have no real sentiment -- watch the confidence)\")\n",
    "print()\n",
    "\n",
    "for text in ambiguous_texts:\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  {result['label']} ({result['score']:.1%})\")\n",
    "    if result['score'] > 0.8:\n",
    "        print(f\"  ^ The model is very confident about a neutral sentence!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives a confident prediction for sentences that have **no real sentiment at all**. \"The meeting was held on Tuesday\" is not positive or negative -- it is just a fact. But the model has to pick a side, and it does so with unwarranted confidence.\n",
    "\n",
    "This is a fundamental problem: **the model cannot say \"I don't know.\"** It always produces an answer, even when the honest answer would be uncertainty.\n",
    "\n",
    "### Stochastic Models: Different Answers Each Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(\"Text generator loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The best way to learn AI is\"\n",
    "\n",
    "print(\"SAME PROMPT, FIVE DIFFERENT ANSWERS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=40,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    output = result[0]['generated_text'][len(prompt):].strip()[:60]\n",
    "    print(f\"  Run {i+1}: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five runs, five different answers. This is a **stochastic** model -- it uses randomness during generation. The sentiment model is **deterministic** -- same input always gives the same output.\n",
    "\n",
    "**The question for both types:** If the model gives a different answer each time, how confident should YOU be in any single answer? And if it always gives the same answer, does that mean it is right?\n",
    "\n",
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** Ask students: \"Which is more dangerous -- a model that admits uncertainty by giving different answers, or a model that always gives the same confident answer even when it's wrong?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Discussion -- Who Gets Hurt? (0:55-1:25)\n",
    "\n",
    "This section is mostly discussion. The code we ran in Parts 1 and 2 gives us evidence. Now we think about what that evidence means for real people.\n",
    "\n",
    "### Scenario 1: Hiring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a company uses AI to screen resumes. The AI reads each resume and gives it a score. Resumes with higher scores get interviews. Resumes with lower scores get rejected automatically.\n",
    "\n",
    "**If the model has the biases we just found:**\n",
    "- What happens to applicants with names the model associates with lower scores?\n",
    "- What happens to people who write in a style the model was not trained on?\n",
    "- Who gets hurt? Who benefits?\n",
    "\n",
    "**Think about:** The company might never know this is happening. They just see a list of \"top candidates\" and assume the AI is being objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: Content Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A social media platform uses AI to flag harmful content. The AI was trained mostly on English text.\n",
    "\n",
    "**Questions:**\n",
    "- What happens to posts in non-English languages? In dialects? In slang?\n",
    "- If the model flags African American Vernacular English (AAVE) as \"toxic\" at higher rates, what is the real-world effect?\n",
    "- Who decides what counts as \"harmful\"? The programmers? The training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: Medical AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medical AI helps doctors diagnose skin conditions from photos. The training data contained mostly photos of lighter skin tones.\n",
    "\n",
    "**Questions:**\n",
    "- What happens when a patient with darker skin uses this tool?\n",
    "- If the AI misses a diagnosis because it was not trained on diverse data, whose fault is that?\n",
    "- How would you even know the AI was failing for certain patients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4: Whose Job Is It?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whose responsibility is it to check for bias?\n",
    "\n",
    "| Stakeholder | Their Argument |\n",
    "|-------------|---------------|\n",
    "| The Programmer | \"I built what I was asked to build. I can't control what's in the data.\" |\n",
    "| The Company | \"We trusted the AI was fair. We didn't know about the bias.\" |\n",
    "| The Government | \"We can't regulate every AI system. Companies should self-regulate.\" |\n",
    "| The Users | \"I just use the tool. I assumed it was fair.\" |\n",
    "\n",
    "**There is no single right answer.** But thinking about these questions is what makes you someone who understands AI, not just someone who uses it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **INSTRUCTOR NOTE:** This should be a real conversation. Don't rush to answers. Let students think and disagree. There are no wrong answers here -- the goal is developing critical thinking about AI systems.\n",
    "\n",
    "### Model Cards: The Bias Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **READ THE MODEL CARD**\n",
    ">\n",
    "> Every model on Hugging Face is supposed to have a \"Bias, Risks, and Limitations\" section in its model card. Let's check a few:\n",
    ">\n",
    "> Go to [huggingface.co/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    ">\n",
    "> - Scroll to \"Bias, Risks, and Limitations.\" What does it say?\n",
    "> - Does it acknowledge the kinds of bias we just found?\n",
    ">\n",
    "> Now check [huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) (the default zero-shot model):\n",
    ">\n",
    "> - What does the model card say about its training data?\n",
    "> - Is there a bias section? If not, what does that absence tell you?\n",
    "> - Compare this card to the sentiment model card -- which is more thorough?\n",
    ">\n",
    "> Not all model cards are equally honest. **A missing bias section is itself a red flag.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swap Slot: Try a Different Zero-Shot Model for Bias Testing\n",
    "\n",
    "The default zero-shot classifier uses `facebook/bart-large-mnli`. Different models may show different biases based on their training data. Try swapping in another zero-shot model and re-running the association test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── SWAP SLOT: Zero-Shot Classification Model ──\n",
    "# Paste the model ID you found on the Hub:\n",
    "\n",
    "my_model = \"PASTE YOUR MODEL ID HERE\"\n",
    "# Example: \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"  (trained on more NLI data)\n",
    "# Example: \"joeddav/xlm-roberta-large-xnli\"  (multilingual)\n",
    "# Example: \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"  (multilingual DeBERTa)\n",
    "\n",
    "# Uncomment the next two lines to load your model:\n",
    "# my_classifier = pipeline(\"zero-shot-classification\", model=my_model)\n",
    "# print(f\"Loaded: {my_model}\")\n",
    "\n",
    "print(\"Swap slot ready. Uncomment above to load your model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Re-run the association test with your model ──\n",
    "# Uncomment and run after loading your model above.\n",
    "\n",
    "# test_sentences = [\n",
    "#     \"The person is a skilled professional.\",\n",
    "#     \"The young man is a skilled professional.\",\n",
    "#     \"The young woman is a skilled professional.\",\n",
    "#     \"The elderly person is a skilled professional.\",\n",
    "# ]\n",
    "#\n",
    "# categories = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"manager\", \"assistant\"]\n",
    "#\n",
    "# print(\"BIAS TEST WITH YOUR MODEL\")\n",
    "# print(\"=\" * 65)\n",
    "#\n",
    "# for text in test_sentences:\n",
    "#     result = my_classifier(text, categories)\n",
    "#     print(f\"\\nText: {text}\")\n",
    "#     print(f\"  Top 3 predictions:\")\n",
    "#     for label, score in zip(result['labels'][:3], result['scores'][:3]):\n",
    "#         print(f\"    {label}: {score:.1%}\")\n",
    "#\n",
    "# print(\"\\nCompare these results to the default model above.\")\n",
    "# print(\"Do different models show different biases?\")\n",
    "\n",
    "print(\"Uncomment the code above after loading your model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## On Your Own (1:40-2:00)\n",
    "\n",
    "### Experiment 1: Design Your Own Bias Test\n",
    "\n",
    "Pick a model (sentiment, zero-shot classifier, or any model from previous sessions) and design a systematic bias test. Change one variable at a time and document the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR BIAS TEST\n",
    "# Step 1: Pick a dimension to test (gender, age, name, occupation, etc.)\n",
    "# Step 2: Create paired sentences that differ ONLY in that dimension\n",
    "# Step 3: Run both through the model and compare\n",
    "\n",
    "my_bias_test = [\n",
    "    (\"REPLACE WITH SENTENCE A\", \"REPLACE WITH SENTENCE B\"),\n",
    "    (\"REPLACE WITH SENTENCE A\", \"REPLACE WITH SENTENCE B\"),\n",
    "    (\"REPLACE WITH SENTENCE A\", \"REPLACE WITH SENTENCE B\"),\n",
    "]\n",
    "\n",
    "print(\"MY BIAS TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for a_text, b_text in my_bias_test:\n",
    "    if \"REPLACE\" in a_text:\n",
    "        continue\n",
    "    result_a = sentiment(a_text)[0]\n",
    "    result_b = sentiment(b_text)[0]\n",
    "    diff = abs(result_a['score'] - result_b['score'])\n",
    "\n",
    "    print(f\"\\nA: {a_text}\")\n",
    "    print(f\"   {result_a['label']} ({result_a['score']:.1%})\")\n",
    "    print(f\"B: {b_text}\")\n",
    "    print(f\"   {result_b['label']} ({result_b['score']:.1%})\")\n",
    "    print(f\"   Difference: {diff:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Confidence Calibration Test\n",
    "\n",
    "Find 10 sentences where you know the \"right\" answer. How often does the model's confidence match reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model's confidence calibration\n",
    "confidence_test = [\n",
    "    (\"REPLACE WITH CLEARLY POSITIVE TEXT\", \"POSITIVE\"),\n",
    "    (\"REPLACE WITH CLEARLY NEGATIVE TEXT\", \"NEGATIVE\"),\n",
    "    (\"REPLACE WITH AMBIGUOUS TEXT\", \"NEUTRAL\"),  # model can't say neutral!\n",
    "    # Add more...\n",
    "]\n",
    "\n",
    "print(\"CONFIDENCE CALIBRATION TEST\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for text, expected in confidence_test:\n",
    "    if \"REPLACE\" in text:\n",
    "        continue\n",
    "    result = sentiment(text)[0]\n",
    "    print(f\"Text: {text[:50]}\")\n",
    "    print(f\"  Expected: {expected}\")\n",
    "    print(f\"  Got: {result['label']} ({result['score']:.1%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Write a Bias Report\n",
    "\n",
    "Based on your experiments today, write a short bias report for one of the models we used.\n",
    "\n",
    "| Section | Your Notes |\n",
    "|---------|-----------|\n",
    "| **Model tested** | |\n",
    "| **What I tested for** | |\n",
    "| **What I found** | |\n",
    "| **Who could be affected** | |\n",
    "| **What should the model creators do about it** | |\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist: Before You Leave\n",
    "\n",
    "- [ ] Ran paired-sentence bias tests on the sentiment model\n",
    "- [ ] Tested zero-shot classification for role-based associations\n",
    "- [ ] Explored how models express (or hide) uncertainty\n",
    "- [ ] Saw stochastic vs. deterministic model behavior\n",
    "- [ ] Discussed real-world scenarios where bias causes harm\n",
    "- [ ] Read the \"Bias, Risks, and Limitations\" section of model cards\n",
    "- [ ] Browsed zero-shot models on the Hub\n",
    "- [ ] Tried the swap slot with a different zero-shot model\n",
    "- [ ] Designed your own bias test\n",
    "- [ ] Saved your work (File > Save a copy in Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking Ahead\n",
    "\n",
    "Next session, we move from single models to **systems**. You have been using one model at a time. But real AI applications chain multiple models together -- the output of one becomes the input of the next. This is powerful, but it introduces a new problem: when one model makes a mistake, every model after it gets worse. We will build multi-model pipelines and learn to think about error cascades.\n",
    "\n",
    "See you next session.\n",
    "\n",
    "---\n",
    "\n",
    "*Youth Horizons AI Researcher Program - Level 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}